{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"00-solution-assignment-5-AI4LE-FINAL.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# <center> Assignment 5 (AI4LR)"],"metadata":{"id":"elcpXOswLRgc"}},{"cell_type":"code","source":["Your_Name = \" \"\n","Your_Email_id = \" \""],"metadata":{"id":"cfmCfF7eiTpk","execution":{"iopub.status.busy":"2022-05-27T05:18:55.007797Z","iopub.execute_input":"2022-05-27T05:18:55.008748Z","iopub.status.idle":"2022-05-27T05:18:55.017066Z","shell.execute_reply.started":"2022-05-27T05:18:55.008659Z","shell.execute_reply":"2022-05-27T05:18:55.016356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Open this notebook in kaggle for easy access to the dataset and related libraries. "],"metadata":{"id":"juQiIIMl4qVz"}},{"cell_type":"code","source":["# importing libraries\n","import tensorflow as tf\n","import keras\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","from PIL import Image"],"metadata":{"id":"f5ayJkkRwkjb","execution":{"iopub.status.busy":"2022-05-27T05:18:55.022350Z","iopub.execute_input":"2022-05-27T05:18:55.022985Z","iopub.status.idle":"2022-05-27T05:18:56.672988Z","shell.execute_reply.started":"2022-05-27T05:18:55.022931Z","shell.execute_reply":"2022-05-27T05:18:56.672167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data \n","\n","To get the dataset for this assignment, got to Add data option and search for https://www.kaggle.com/datasets/datatangai/people-with-occlusion-and-multipose-face-data in Search URL field.\n","\n","Import this dataset and there is only one image in this dataset. We are going to use this image only. "],"metadata":{"id":"d72b-W8fFaKI"}},{"cell_type":"code","source":["img_path = \"../input/people-with-occlusion-and-multipose-face-data/3.png\"\n","\n","img = Image.open(img_path)\n","\n","img"],"metadata":{"id":"T808--eV4qV0","execution":{"iopub.status.busy":"2022-05-27T05:18:56.676045Z","iopub.execute_input":"2022-05-27T05:18:56.676541Z","iopub.status.idle":"2022-05-27T05:18:57.782467Z","shell.execute_reply.started":"2022-05-27T05:18:56.676514Z","shell.execute_reply":"2022-05-27T05:18:57.781414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q1 (1 Point) \n","\n","Convert the images \"img\" into an array and store that array into 'img_arr' variable.\n","\n","Plot the img_arr using plt\n"],"metadata":{"id":"QAhrBKqmjuHB"}},{"cell_type":"code","source":["# Complete the following code\n","\n","img_arr = np.asarray(img)\n","print(type(img_arr))\n","print(img_arr.shape)\n","\n","# plot img_arr\n","\n","plt.imshow(img_arr, interpolation='nearest')\n","plt.show()"],"metadata":{"id":"ShSSOgg350kW","execution":{"iopub.status.busy":"2022-05-27T05:18:57.785787Z","iopub.execute_input":"2022-05-27T05:18:57.786285Z","iopub.status.idle":"2022-05-27T05:18:58.077850Z","shell.execute_reply.started":"2022-05-27T05:18:57.786236Z","shell.execute_reply":"2022-05-27T05:18:58.077121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q2 (2 Points)\n","Run the below code.\n","\n","Do you see anything abnormal with the shape of the above image? If yes, state the reason. "],"metadata":{"id":"KqNymgaxpk1i"}},{"cell_type":"code","source":["print(\"Image shape:\", img_arr.shape)\n","\n","## print your reason here\n","\n","reason = \"four channel image\"\n","\n","print(reason)"],"metadata":{"id":"23zzRR39pj-X","execution":{"iopub.status.busy":"2022-05-27T05:18:58.079260Z","iopub.execute_input":"2022-05-27T05:18:58.079782Z","iopub.status.idle":"2022-05-27T05:18:58.085837Z","shell.execute_reply.started":"2022-05-27T05:18:58.079746Z","shell.execute_reply":"2022-05-27T05:18:58.084837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q3 (6 Points)\n","The above image \"img_arr\" is a combination of ten different images, separate all ten images using numpy and show all of them using plt.subplot.\n","\n","(the size and shape of individual images should remain the same)"],"metadata":{"id":"0wj839JEq2vN"}},{"cell_type":"code","source":["# first we are converting our four channels image into three channels image (easy to use and no extra complexities)\n","'''Do not make any change in this below line of code.'''\n","img_arr = img_arr[:, :, :3]\n","\n","# Write your code here--------------------------->\n","height = np.size(img_arr, 0)\n","width = np.size(img_arr, 1)\n","width_img = width//5\n","height_img = height//2\n","image1 = img_arr[0:height_img,:-(4*(width_img))]\n","image2 = img_arr[0:height_img,width_img:-(3*(width_img))]\n","image3 = img_arr[0:height_img,2*(width_img):-(2*(width_img))]\n","image4 = img_arr[0:height_img,3*(width_img):-(width_img)]\n","image5 = img_arr[0:height_img,4*(width_img):]\n","image6 = img_arr[height_img:,:-(4*(width_img))]\n","image7 = img_arr[height_img:,width_img:-(3*(width_img))]\n","image8 = img_arr[height_img:,2*(width_img):-(2*(width_img))]\n","image9 = img_arr[height_img:,3*(width_img):-(width_img)]\n","image10 = img_arr[height_img:,4*(width_img):]\n","fig=plt.figure(figsize=(15, 15))\n","ax1=fig.add_subplot(2,5,1)\n","ax1.imshow(image1)\n","ax2=fig.add_subplot(2,5,2)\n","ax2.imshow(image2)\n","ax3=fig.add_subplot(2,5,3)\n","ax3.imshow(image3)\n","ax4=fig.add_subplot(2,5,4)\n","ax4.imshow(image4)\n","ax5=fig.add_subplot(2,5,5)\n","ax5.imshow(image5)\n","ax6=fig.add_subplot(2,5,6)\n","ax6.imshow(image6)\n","ax7=fig.add_subplot(2,5,7)\n","ax7.imshow(image7)\n","ax8=fig.add_subplot(2,5,8)\n","ax8.imshow(image8)\n","ax9=fig.add_subplot(2,5,9)\n","ax9.imshow(image9)\n","ax10=fig.add_subplot(2,5,10)\n","ax10.imshow(image10)\n","plt.show()\n","\n","\n","\n"],"metadata":{"id":"G979ANNupyoY","execution":{"iopub.status.busy":"2022-05-27T05:18:58.087644Z","iopub.execute_input":"2022-05-27T05:18:58.088498Z","iopub.status.idle":"2022-05-27T05:18:59.526120Z","shell.execute_reply.started":"2022-05-27T05:18:58.088459Z","shell.execute_reply":"2022-05-27T05:18:59.525416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q4 (2 Points)\n","imgGray = ** 0.2989 * R + 0.5870 * G + 0.1140 * B **. \n","\n","this is the formula to convert any colored images to greyscale. \n","\n","\n","Convert \"first\" into greyscale using above formula and store it in \"first_bw\""],"metadata":{"id":"wYxLSwSWrgey"}},{"cell_type":"code","source":["first = img_arr[:500, :290, :]\n","\n","plt.figure(figsize = (5,10))\n","plt.imshow(first)\n","plt.show()\n","\n","\n","\n","## Write your code here\n","R, G, B = first[:,:,0], first[:,:,1], first[:,:,2]\n","first_bw = 0.2989 * R + 0.5870 * G + 0.1140 * B\n","plt.imshow(first_bw, cmap='gray')\n","plt.show()"],"metadata":{"id":"482aUQZeraMd","execution":{"iopub.status.busy":"2022-05-27T05:18:59.527305Z","iopub.execute_input":"2022-05-27T05:18:59.528023Z","iopub.status.idle":"2022-05-27T05:18:59.962904Z","shell.execute_reply.started":"2022-05-27T05:18:59.527993Z","shell.execute_reply":"2022-05-27T05:18:59.962115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q5 (6 Points)\n","Find out the effects of kernel1 and kernel2 on the \"first\" using cv2.filter2D function. \n","\n","Plot and State the difference between effect1 and effect2.\n"],"metadata":{"id":"kiStl1oc6O5O"}},{"cell_type":"code","source":["kernel1 = np.array([[-1,-1,-1],\n","                    [0,0,0],\n","                    [1,1,1]])\n","\n","kernel2 = kernel1.transpose()\n","\n","\n","effect1 = cv2.filter2D(src = first, kernel = kernel1, ddepth = -1)\n","effect2 = cv2.filter2D(src = first, kernel = kernel2, ddepth = -1)\n","\n","# plot effect1 and effect2 using plt.subplots\n","plt.imshow(effect1)\n","plt.show()\n","plt.imshow(effect2)\n","plt.show()\n","\n","\n","\n","# state the difference\n","\n","difference = \"Kernel 1 This mask will find edges in the horizontal direction - because the zeros column is in the horizontal direction. This mask will prominent the horizontal edges in an image. It calculates the difference among the pixel intensities of a particular edge. As the center row consists of zeros it does not include the original values of edge in the image but rather it calculates the difference of above and below pixel intensities of the particular edge. Thus increasing the sudden change of intensities and making the edge more visible. Kernel 2 The mask will find the edges in vertical direction - because the zeros column is in the vertical direction. When we apply this mask on the image it makes the vertical edges prominent. It simply works like as first order derivate and calculates the difference of pixel intensities in a edge region. As the center column is of zero so it does not include the original values of an image but rather it calculates the difference of right and left pixel values around that edge. This increases the edge intensity and it becomes enhanced compared to the original image.Together applied to an image, it makes up the Prewitt operatorBoth the above masks follow the principle of derivate mask. Both masks have opposite sign in them and both masks sum equals to zero. \"\n","\"\"\"\n","Kernel 1\n","This mask will find edges in the horizontal direction - because the zeros column is in the horizontal direction. \n","This mask will prominent the horizontal edges in an image. It calculates the difference among the pixel intensities of a particular edge. \n","As the center row consists of zeros it does not include the original values of edge in the image but rather it calculates the difference of \n","above and below pixel intensities of the particular edge. Thus increasing the sudden change of intensities and making the edge more visible. \n","\n","Kernel 2 \n","The mask will find the edges in vertical direction - because the zeros column is in the vertical direction. \n","When we apply this mask on the image it makes the vertical edges prominent. It simply works like as first order derivate and \n","calculates the difference of pixel intensities in a edge region. As the center column is of zero so it does not include the \n","original values of an image but rather it calculates the difference of right and left pixel values around that edge. \n","This increases the edge intensity and it becomes enhanced compared to the original image.\n","\n","\n","Together applied to an image, it makes up the Prewitt operator\n","Both the above masks follow the principle of derivate mask. Both masks have opposite sign in them and both masks sum equals to zero. \n","\"\"\"\n","\n","print(difference)\n"],"metadata":{"id":"TbY77UkX67Xv","execution":{"iopub.status.busy":"2022-05-27T05:18:59.965179Z","iopub.execute_input":"2022-05-27T05:18:59.965830Z","iopub.status.idle":"2022-05-27T05:19:00.322380Z","shell.execute_reply.started":"2022-05-27T05:18:59.965790Z","shell.execute_reply":"2022-05-27T05:19:00.321596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q6 (4 Points)\n","\n","State differences between -  \n","\n","> Feature extraction and finetuning in transfer learning.\n","\n","> Hidden layer in a neural network and Head layer in transfer learning "],"metadata":{"id":"Qh-Tu3DrGf7Y"}},{"cell_type":"code","source":["# start your answer here\n","\n","answer1 = \"Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features.\" \n","\n","answer2 = \"Transfer learning consists of taking features learned on one problem and leveraging them on a similar problem. After we import layers from a previously trained model we add new trainable layers on top of the frozen layers. In fine-tuning we unfreeze the entire model or part of it and re-train it on the new data with a very low learning rate. This incrementally adapts the pretrained features to the new data\"\n","\n","print(answer1)\n","print()\n","print(answer2)\n"],"metadata":{"id":"R1ja-IvR4d34","execution":{"iopub.status.busy":"2022-05-27T05:19:00.323660Z","iopub.execute_input":"2022-05-27T05:19:00.324312Z","iopub.status.idle":"2022-05-27T05:19:00.330761Z","shell.execute_reply.started":"2022-05-27T05:19:00.324272Z","shell.execute_reply":"2022-05-27T05:19:00.329950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q7 (1 point)\n","\n","Write the formula for IOU score using numpy."],"metadata":{"id":"s7S2hsT-7eu-"}},{"cell_type":"code","source":["# Print your answer \n","\n","formula = \"np.sum(intersection) / np.sum(union)\" \n","\n","print(formula)"],"metadata":{"id":"_qcETSSR8866","execution":{"iopub.status.busy":"2022-05-27T05:19:00.332146Z","iopub.execute_input":"2022-05-27T05:19:00.333286Z","iopub.status.idle":"2022-05-27T05:19:00.339116Z","shell.execute_reply.started":"2022-05-27T05:19:00.333228Z","shell.execute_reply":"2022-05-27T05:19:00.338094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q8 (3 points)\n","\n","For below model, \n","\n","For a Convolutional layer in neural network, below values are given\n","\n","> Input shape = (64, 64, 3)\n","\n","> Convolutional filter size = (3,3)\n","\n","> Number of Conv Filters = 16\n","\n","> padding = \"same\"\n","\n","> stride = 1\n","\n","\n","Find out the **output shape** after this layer, **number of total trainable paramaters** and **computational cost**.\n","\n","Hint: Total trainable parameters is the sum of total number of weights and biases in the layer.\n","\n","Hint: Computational cost is the total number of matrix multiplications"],"metadata":{"id":"0mL9NMduQFg5"}},{"cell_type":"code","source":["from keras.engine.sequential import Sequential\n","from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout\n","\n","model = Sequential([\n","                    Input(shape = (500, 290, 3)),\n","                    Conv2D(16, (3,3), activation = 'relu'),\n","                    MaxPooling2D(2,2),\n","                    Conv2D(32, (3,3), activation = 'relu'),\n","                    MaxPooling2D(2,2),\n","                    Conv2D(64, (3,3), activation = 'relu'),\n","                    MaxPooling2D(2,2),\n","                    Dense(256, activation = \"relu\"),\n","                    Dropout(0.5),\n","                    Dense(5, activation = \"sigmoid\")\n","])\n","\n","# write your answer here\n","\n","out_shape = [None, 60, 34, 5]\n","\n","train_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","print(train_params)\n","\n","# write a function for this looping over layers, calculating for each layer\n","no_of_ip_ch = [3, 16, 32, 64, 256]\n","no_of_op_ch = [16, 32, 64, 256, 5]\n","filter_size = 3\n","comp_cost = 0\n","for i in range(5):\n","    if type(model.layers[i]) == keras.layers.Conv2D:\n","        no_of_weights = no_of_ip_ch[i] * filter_size * filter_size * no_of_op_ch[i]        \n","    else:\n","        no_of_weights = no_of_ip_ch[i] * no_of_op_ch[i]\n","    no_of_biases = no_of_op_ch[i]\n","    layer_total = no_of_weights + no_of_biases\n","    comp_cost += layer_total\n","\n","print(\"Output shape:\", out_shape)\n","print(\"Trainable parameters:\", train_params)\n","print(\"Computational Cost:\", comp_cost)\n","\n","# trainableParams = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n","# nonTrainableParams = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n","# totalParams = trainableParams + nonTrainableParams "],"metadata":{"id":"HUVQCUDXP_Rm","execution":{"iopub.status.busy":"2022-05-27T05:19:00.341007Z","iopub.execute_input":"2022-05-27T05:19:00.341414Z","iopub.status.idle":"2022-05-27T05:19:01.382810Z","shell.execute_reply.started":"2022-05-27T05:19:00.341377Z","shell.execute_reply":"2022-05-27T05:19:01.382014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Q9 (5 points)\n","\n","1. Write down the name of the model that used dropout for preventing overfitting for the first time?\n","\n","2. How did GoogleNet manage to reduce its number of parameters despite having a deep heirarchy of conv layers?\n","\n","3. Which network made 3-by-3 filters state-of-the-art?\n","\n","4. How does VGGNet ensure growing depth despit shrinking spatial dimensions deep into the network hierarchy?\n","\n","5. What does Deconvnet do? Which network introduced it? "],"metadata":{"id":"1gL2_7F2jxec"}},{"cell_type":"markdown","source":["Ans 9.1: AlexNet.\n","\n","\n","9.2: They used an average pool to go from a 7x7x1024 volume to a 1x1x1024 volume.\n","\n","\n","9.3: VGGNet.\n","\n","\n","9.4: Increasing the number of filters as we go down the hierarchy keeps the output dimensions high - the number of filters is doubled after every maxpooling layer.\n","\n","\n","9.5: It was introduced by ZFNet - it matches features to pixels - exactly opposite of what a convolution does. "],"metadata":{"id":"X7OY3fTXynBC"}},{"cell_type":"markdown","source":["## Q10 (20 points)\n","\n","Use the Iris dataset to classify iris flowers into three classes of species.\n","https://www.kaggle.com/datasets/jeffheaton/iris-computer-vision\n","\n","Develop a transfer learning network by following these steps:\n","1. Derive a base model using VGGNet - any configuration of your choice. \n","\n","2. Augment your data set by defining data augmentation layers - use at least 2 different augmentation techniques.\n","\n","3. Build your network with the augmentation layers and the base model from VGGNet.\n","\n","4. Add a pooling layer and a dense layer at the end to get a single prediction per image.\n","\n","5. Compile your model using categorical cross entropy loss, adam optimizer, and accuracy metric.\n","\n","6. Train your network twice - once with dropout and another time without dropout. \n","\n","7. Plot training and validation losses and accuracies for both cases of point #6. \n"],"metadata":{"id":"wJMt9xKSTQsf"}},{"cell_type":"markdown","source":["## Thank you for completing all the questions!\n","#### Download your notebook and submit it in the google form"],"metadata":{"id":"wIiqmkDHRLwC"}},{"cell_type":"code","source":["# getting path to our root directory\n","data_dir = '../input/iris-computer-vision'\n","\n","batch_size = 32\n","img_height = 160 # original images are 256-by-256, but we will read in the images with a reduced size -> 160-by-160\n","img_width = 160\n","IMG_SIZE = (160, 160)\n","\n","train_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","val_ds = tf.keras.utils.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"],"metadata":{"id":"whmuWZ5AlMzL","execution":{"iopub.status.busy":"2022-05-27T05:19:01.384236Z","iopub.execute_input":"2022-05-27T05:19:01.384830Z","iopub.status.idle":"2022-05-27T05:19:01.678060Z","shell.execute_reply.started":"2022-05-27T05:19:01.384793Z","shell.execute_reply":"2022-05-27T05:19:01.676503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.RandomFlip('horizontal'),\n","  tf.keras.layers.RandomRotation(0.2),\n","])\n","\n","preprocess_input = tf.keras.applications.vgg19.preprocess_input\n","\n","IMG_SHAPE = IMG_SIZE + (3,)\n","base_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n","                                               include_top=False,\n","                                               weights='imagenet')"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:01.681482Z","iopub.execute_input":"2022-05-27T05:19:01.681760Z","iopub.status.idle":"2022-05-27T05:19:02.056833Z","shell.execute_reply.started":"2022-05-27T05:19:01.681734Z","shell.execute_reply":"2022-05-27T05:19:02.055991Z"},"trusted":true,"id":"189Ly9-t-MPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_batch, label_batch = next(iter(train_ds))\n","feature_batch = base_model(image_batch)\n","print(feature_batch.shape)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:02.058003Z","iopub.execute_input":"2022-05-27T05:19:02.058399Z","iopub.status.idle":"2022-05-27T05:19:03.798544Z","shell.execute_reply.started":"2022-05-27T05:19:02.058362Z","shell.execute_reply":"2022-05-27T05:19:03.797698Z"},"trusted":true,"id":"VwLNa7Go-MPz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model.trainable = False"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:03.802681Z","iopub.execute_input":"2022-05-27T05:19:03.803639Z","iopub.status.idle":"2022-05-27T05:19:03.809262Z","shell.execute_reply.started":"2022-05-27T05:19:03.803598Z","shell.execute_reply":"2022-05-27T05:19:03.808226Z"},"trusted":true,"id":"FXRzrWFf-MP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","feature_batch_average = global_average_layer(feature_batch)\n","print(feature_batch_average.shape)\n","\n","prediction_layer = tf.keras.layers.Dense(1)\n","prediction_batch = prediction_layer(feature_batch_average)\n","print(prediction_batch.shape)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:03.810611Z","iopub.execute_input":"2022-05-27T05:19:03.811350Z","iopub.status.idle":"2022-05-27T05:19:03.830520Z","shell.execute_reply.started":"2022-05-27T05:19:03.811181Z","shell.execute_reply":"2022-05-27T05:19:03.828940Z"},"trusted":true,"id":"M2EF72dy-MP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tf.keras.Input(shape=(160, 160, 3))\n","x = data_augmentation(inputs)\n","x = preprocess_input(x)\n","x = base_model(x, training=False)\n","x = global_average_layer(x)\n","\n","outputs = prediction_layer(x)\n","model = tf.keras.Model(inputs, outputs)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:03.831630Z","iopub.execute_input":"2022-05-27T05:19:03.832088Z","iopub.status.idle":"2022-05-27T05:19:04.111112Z","shell.execute_reply.started":"2022-05-27T05:19:03.832055Z","shell.execute_reply":"2022-05-27T05:19:04.110299Z"},"trusted":true,"id":"vgB4DHZu-MP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n","              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","initial_epochs = 5"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:04.112368Z","iopub.execute_input":"2022-05-27T05:19:04.112729Z","iopub.status.idle":"2022-05-27T05:19:04.127811Z","shell.execute_reply.started":"2022-05-27T05:19:04.112684Z","shell.execute_reply":"2022-05-27T05:19:04.126971Z"},"trusted":true,"id":"-gkbI5f3-MP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_ds,\n","                    epochs=initial_epochs,\n","                    validation_data=val_ds)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:04.129019Z","iopub.execute_input":"2022-05-27T05:19:04.129913Z","iopub.status.idle":"2022-05-27T05:19:15.082647Z","shell.execute_reply.started":"2022-05-27T05:19:04.129873Z","shell.execute_reply":"2022-05-27T05:19:15.081780Z"},"trusted":true,"id":"AgFtFWp8-MP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:15.084143Z","iopub.execute_input":"2022-05-27T05:19:15.084542Z","iopub.status.idle":"2022-05-27T05:19:15.397090Z","shell.execute_reply.started":"2022-05-27T05:19:15.084505Z","shell.execute_reply":"2022-05-27T05:19:15.396311Z"},"trusted":true,"id":"PMYmgeJl-MP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tf.keras.Input(shape=(160, 160, 3))\n","z = data_augmentation(inputs)\n","z = preprocess_input(z)\n","z = base_model(z, training=False)\n","z = global_average_layer(z)\n","z = tf.keras.layers.Dropout(0.2)(z)\n","outputs = prediction_layer(z)\n","model_dropout = tf.keras.Model(inputs, outputs)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:15.398476Z","iopub.execute_input":"2022-05-27T05:19:15.399001Z","iopub.status.idle":"2022-05-27T05:19:15.538498Z","shell.execute_reply.started":"2022-05-27T05:19:15.398961Z","shell.execute_reply":"2022-05-27T05:19:15.537568Z"},"trusted":true,"id":"UzHNISjh-MP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_learning_rate_dropout = 0.0001\n","\n","model_dropout.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate_dropout),\n","              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","initial_epochs = 5\n","\n","history_dropout = model_dropout.fit(train_ds,\n","                    epochs=initial_epochs,\n","                    validation_data=val_ds)"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:19:58.261536Z","iopub.execute_input":"2022-05-27T05:19:58.261902Z","iopub.status.idle":"2022-05-27T05:20:05.259661Z","shell.execute_reply.started":"2022-05-27T05:19:58.261871Z","shell.execute_reply":"2022-05-27T05:20:05.258770Z"},"trusted":true,"id":"XCZHTKNO-MP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["acc = history_dropout.history['accuracy']\n","val_acc = history_dropout.history['val_accuracy']\n","\n","loss = history_dropout.history['loss']\n","val_loss = history_dropout.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()),1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0,1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2022-05-27T05:20:09.182652Z","iopub.execute_input":"2022-05-27T05:20:09.183027Z","iopub.status.idle":"2022-05-27T05:20:09.521657Z","shell.execute_reply.started":"2022-05-27T05:20:09.182998Z","shell.execute_reply":"2022-05-27T05:20:09.520298Z"},"trusted":true,"id":"e5_YkfP9-MP3"},"execution_count":null,"outputs":[]}]}