{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Best Practices for Image segmentation (W7S2)","metadata":{"id":"kD6lbSNVLszk"}},{"cell_type":"markdown","source":"#### This session will provide you brief introduction to some best practices to improve performance of your Image segmentation model","metadata":{"id":"pHL8t-2oLszp"}},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt","metadata":{"id":"3eItJm1hLszr","execution":{"iopub.status.busy":"2022-06-01T13:52:48.505508Z","iopub.execute_input":"2022-06-01T13:52:48.506220Z","iopub.status.idle":"2022-06-01T13:52:49.117568Z","shell.execute_reply.started":"2022-06-01T13:52:48.506097Z","shell.execute_reply":"2022-06-01T13:52:49.116640Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"'''\nHere load_data function is called. This will load the dataset paths and \nsplit it into X_train, X_test, y_train, y_test '''\n\nimg_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\nmask_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n\n\n# let's get the list of image paths and mask paths in sorted order from the given directory respectively\nimages = [os.path.join(img_dir, x) for x in sorted(os.listdir(img_dir))]\nmasks = [os.path.join(mask_dir, x) for x in sorted(os.listdir(mask_dir))]\n","metadata":{"id":"ofv9tYqEkO5V","execution":{"iopub.status.busy":"2022-06-01T13:53:13.263388Z","iopub.execute_input":"2022-06-01T13:53:13.263850Z","iopub.status.idle":"2022-06-01T13:53:13.821846Z","shell.execute_reply.started":"2022-06-01T13:53:13.263812Z","shell.execute_reply":"2022-06-01T13:53:13.821054Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"s_path = images[0]\n\nsample = imread(s_path)\n\nplt.imshow(sample)\n\nplt.show()","metadata":{"id":"Eivn-ZTpRXWG","execution":{"iopub.status.busy":"2022-06-01T14:05:32.021836Z","iopub.execute_input":"2022-06-01T14:05:32.023084Z","iopub.status.idle":"2022-06-01T14:05:32.283009Z","shell.execute_reply.started":"2022-06-01T14:05:32.023024Z","shell.execute_reply":"2022-06-01T14:05:32.282206Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Difference of Gaussians\n\nhttps://scikit-image.org/docs/dev/api/skimage.filters.html?highlight=gaussian#difference-of-gaussians\n\nThe input array is blurred with two Gaussian kernels of differing sigmas to produce two intermediate, filtered images. The more-blurred image is then subtracted from the less-blurred image. The final output image will therefore have had high-frequency components attenuated by the smaller-sigma Gaussian, and low frequency components will have been removed due to their presence in the more-blurred intermediate.","metadata":{"id":"iN9cFTOFRXWG"}},{"cell_type":"code","source":"import skimage\n\n# Find features between low_sigma and high_sigma in size.\npre = skimage.filters.difference_of_gaussians(sample, low_sigma = 2, high_sigma = 8)\n\nplt.imshow(pre)\nplt.show()","metadata":{"id":"ldw3ht4lRXWG","execution":{"iopub.status.busy":"2022-06-01T14:05:37.580926Z","iopub.execute_input":"2022-06-01T14:05:37.582006Z","iopub.status.idle":"2022-06-01T14:05:38.068224Z","shell.execute_reply.started":"2022-06-01T14:05:37.581956Z","shell.execute_reply":"2022-06-01T14:05:38.067169Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Ensure that all the images have the same orientation\n\nhttps://albumentations.ai/","metadata":{"id":"NW_XBy0uRXWH"}},{"cell_type":"code","source":"import albumentations as A\nimport cv2\n\n# Declare an augmentation pipeline\ntransform = A.Compose([\n    A.RandomCrop(width=256, height=256),\n    A.HorizontalFlip(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n])\n\n# Read an image with OpenCV and convert it to the RGB colorspace\nimage = cv2.imread(s_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Augment an image\ntransformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n\nplt.imshow(transformed_image)","metadata":{"id":"ADniPDy_RXWH","execution":{"iopub.status.busy":"2022-06-01T14:14:33.322748Z","iopub.execute_input":"2022-06-01T14:14:33.324865Z","iopub.status.idle":"2022-06-01T14:14:33.686679Z","shell.execute_reply.started":"2022-06-01T14:14:33.324830Z","shell.execute_reply":"2022-06-01T14:14:33.685677Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Why Albumentations?\n\nhttps://github.com/albumentations-team/albumentations\n\n* Albumentations supports all common computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, and pose estimation.\n* The library provides a simple unified API to work with all data types: images (RBG-images, grayscale images, multispectral images), segmentation masks, bounding boxes, and keypoints.\n*     The library contains more than 70 different augmentations to generate new training samples from the existing data.\n*     Albumentations is fast. They benchmark each new release to ensure that augmentations provide maximum speed.\n*     It works with popular deep learning frameworks such as PyTorch and TensorFlow. By the way, Albumentations is a part of the PyTorch ecosystem.\n*     Written by experts. The authors have experience both working on production computer vision systems and participating in competitive machine learning. Many core team members are Kaggle Masters and Grandmasters.\n*     The library is widely used in industry, deep learning research, machine learning competitions, and open source projects.\n","metadata":{"id":"7YZ-UHkmRXWI"}},{"cell_type":"markdown","source":"## Contrast Streching and Histogram Equalization","metadata":{"id":"teaFdGOyRXWJ"}},{"cell_type":"code","source":"# Contrast stretching - improve the contrast in an image by \n# `stretching' the range of intensity values it contains to span a desired range of values\np2, p98 = np.percentile(sample, (2, 98))\nimg_rescale = skimage.exposure.rescale_intensity(sample, in_range=(p2, p98))\n\n# Equalization - spreads out the most frequent intensity values\nimg_eq = skimage.exposure.equalize_hist(sample)\n\n# plotting\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (20, 8))\n\nax1.set_title(\"Original\")\nax1.imshow(sample)\n\nax2.set_title(\"Contrast streching\")\nax2.imshow(img_rescale)\n\nax3.set_title(\"Histogram Equalization\")\nax3.imshow(img_eq)\n\nplt.show()","metadata":{"id":"viiwW9X2RXWJ","execution":{"iopub.status.busy":"2022-06-01T14:20:34.665151Z","iopub.execute_input":"2022-06-01T14:20:34.665676Z","iopub.status.idle":"2022-06-01T14:20:35.384040Z","shell.execute_reply.started":"2022-06-01T14:20:34.665628Z","shell.execute_reply":"2022-06-01T14:20:35.383087Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"While histogram equalization has the advantage that it requires no parameters, it sometimes yields unnatural looking images.\n\nIn Contrast stretching, the image is rescaled to include all intensities that fall within the 2nd and 98th percentiles.","metadata":{"id":"ffcegrkORXWK"}},{"cell_type":"markdown","source":"## Pseudo labeling (Semi Supervised learning)\n\nhttps://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af\n\nIt will be useful in those cases when you have labels for a very small subset of data but not for the other. In that case, you can generate model training pipeline to use outputs from model unlabeled data to act as labels and train with unlabeled data also.\n\n\n","metadata":{"id":"-x4xRzrNRXWK"}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1118/1*Yk_mGPVIJgkIhf0gWo7PTA.png)","metadata":{"id":"6xskFKKsRXWL"}},{"cell_type":"markdown","source":"## Ensemble with different SOTA models. \n\nhttps://www.kaggle.com/c/inclusive-images-challenge/discussion/72450\n\nhttps://github.com/IVPLatNU/DeepCovidXR\n\n![](https://miro.medium.com/max/1400/1*MxD8Kn_Rn9p_Au4MOGgsmg.png)\n","metadata":{"id":"KJoBGXa1RXWL"}},{"cell_type":"markdown","source":"## Different metrics to use:-\n\n* https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n* https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n* https://www.kaggle.com/code/yassinealouini/all-the-segmentation-metrics/notebook","metadata":{"id":"GmFGRzkpRXWM"}},{"cell_type":"markdown","source":"## Some more references for future:-\n\n* https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0255397\n* https://neptune.ai/blog/image-segmentation\n* https://www.v7labs.com/blog/panoptic-segmentation-guide\n* https://www.v7labs.com/blog/image-annotation-guide\n* And always: https://spartificial.com/resources\n","metadata":{"id":"AIBvOiItRXWM"}},{"cell_type":"markdown","source":"# <center> And by the way have you seen convolved foxes before? :)","metadata":{"id":"JWN3T3OtRXWM"}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/vecxoz/kag18_inclusive_images/master/explore/6edc75f64a451387.jpg)","metadata":{"id":"EIPwwS_ZRXWM"}},{"cell_type":"code","source":"","metadata":{"id":"3ajAnaiZkO5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"q26M3dwpkO5n"},"execution_count":null,"outputs":[]}]}