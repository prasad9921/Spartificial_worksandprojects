{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part 2 - W6-S1 - notebook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <center>Week-6: Starting with Semantic Segmentation"],"metadata":{"id":"ns5OL2MIOBCz"}},{"cell_type":"markdown","source":["# <center> Transpose Convolution"],"metadata":{"id":"katgEikoOs2C"}},{"cell_type":"markdown","source":["#### Transposed Convolutions are used to upsample the input feature map to a desired output feature map using some learnable parameters.\n","\n"],"metadata":{"id":"9M1Q0NmUTJmz"}},{"cell_type":"markdown","source":["## Why do we need transpose convolution?\n","\n","\n","\n","\n","One of the best ways for us to gain some intuition is by looking at examples from Computer Vision that use the transposed convolution. Most of these examples start with a series of regular convolutions to compress the input data into an abstract spatial representation, and then use transposed convolutions to decompress the abstract representation into something of use.\n","\n","\n","![](https://miro.medium.com/max/1400/1*1OabPemOWCLrpCwIUmIsCg.png)\n","\n","\n","A convolutional auto-encoder is tasked with recreating its input image, after passing intermediate results through a ‘bottleneck’ of a limited size. Uses of auto-encoders include compression, noise removal, colourisation and in-painting. Success depends on being able to learn dataset specific compression in the convolution kernels and dataset specific decompression in the transposed convolution kernels. Why stop there though?\n","\n","\n","## Super Resolution\n","![](https://miro.medium.com/max/1400/1*eTMo62iQxKp9aR3b5iG14w.png)\n","\n","\n","With ‘super resolution’ the objective is to upscale the input image to higher resolutions, so transposed convolutions can be used as an alternative to classical methods such as bicubic interpolation. Similar arguments to convolutions using learnable kernels over hand crafted kernels apply here.\n","\n","## Semantic Segmentation\n","\n","\n","Semantic segmentation is an example of using transposed convolution layers to decompress the abstract representation into a different domain (from the RGB image input). We output a class for each pixel of the input image, and then just for visualisation purposes, we render each class as a distinct colour (e.g. the person class shown in red, cars in dark blue, etc.).\n","\n","\n","![](https://miro.medium.com/max/1400/1*KICInky28yGdU9T45kIL5Q.jpeg)"],"metadata":{"id":"DYzUi4PGc5HL"}},{"cell_type":"markdown","source":["1. The CNN layers we have seen so far, such as convolutional layers and pooling layers typically reduce (downsample) the spatial dimensions (height and width) of the input, or keep them unchanged. \n","\n","2. In semantic segmentation that classifies at pixel-level, it will be convenient if the spatial dimensions of the input and output are the same. For example, the channel dimension at one output pixel can hold the classification results for the input pixel at the same spatial position.\n","\n","3. To achieve this, especially after the spatial dimensions are reduced by CNN layers, we can use another type of CNN layers that can increase (upsample) the spatial dimensions of intermediate feature maps. \n","\n","4. In this session, we will introduce transposed convolution, which is also called fractionally-strided convolution, for reversing downsampling operations by the convolution."],"metadata":{"id":"efVG-Tz9RDYE"}},{"cell_type":"markdown","source":["# Now let's understand how transpose convolution works \n","\n"],"metadata":{"id":"RFhD-P9Qep2p"}},{"cell_type":"markdown","source":["### The basic operation that goes in a transposed convolution is explained below:\n","### 1. Consider a 2x2 encoded feature map which needs to be upsampled to a 3x3 feature map"],"metadata":{"id":"7Y19GH5Ccxvz"}},{"cell_type":"markdown","source":["![](https://miro.medium.com/max/97/1*BMJnnOKPhK8hoFP6sQ9edQ.png) \n","\n","Input Feature map\n","\n","![](https://miro.medium.com/max/252/1*VxtMdM-DsGwIa51GyDx-XQ.png)\n","\n","Output feature shape\n","\n","\n","\n"],"metadata":{"id":"HePsk9b6Tj8h"}},{"cell_type":"markdown","source":["### 2. We take a kernel of size 2x2 with unit stride and zero padding.\n","\n","![](https://miro.medium.com/max/102/1*e6UnrcsFRaOidCq7mwJpTA.png)\n","\n","kernel of size 2*2"],"metadata":{"id":"K0X3rUS2ZPlU"}},{"cell_type":"markdown","source":["### 3. Now we take the upper left element of the input feature map and multiply it with every element of the kernel\n","\n","![](https://miro.medium.com/max/429/1*7hVid7EAqCPkG6sEjHMI5w.png)\n","\n"],"metadata":{"id":"dVBnkoFhZhM8"}},{"cell_type":"markdown","source":["### 4. Similarly, we do it for all the remaining elements of the input feature map\n","\n","![](https://miro.medium.com/max/700/1*yxBd_pCiEVVwEQFmc-Heog.png)"],"metadata":{"id":"9rKOfEFyaW_D"}},{"cell_type":"markdown","source":["### 5. As you can see, some of the elements of the resulting upsampled feature maps are over-lapping. To solve this issue, we simply add the elements of the over-lapping positions\n","\n","![](https://miro.medium.com/max/700/1*faRskFzI7GtvNCLNeCN8cg.png)\n","\n","\n"],"metadata":{"id":"63TJ9CHObYnW"}},{"cell_type":"markdown","source":["## 6. The resulting output will be the final upsampled feature map having the required spatial dimensions of 3x3\n","\n","![](https://miro.medium.com/max/790/1*Lpn4nag_KRMfGkx1k6bV-g.gif)"],"metadata":{"id":"Bw17loKScMYq"}},{"cell_type":"code","source":["def trans_conv(X, K):\n","    h, w = K.shape\n","    Y = np.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n","    for i in range(X.shape[0]):\n","        for j in range(X.shape[1]):\n","            Y[i: i + h, j: j + w] += X[i, j] * K\n","    return Y"],"metadata":{"id":"ipLCMGRQucrc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.array([[0.0, 1.0], [2.0, 3.0]])\n","K = np.array([[0.0, 1.0], [2.0, 3.0]])\n","trans_conv(X, K)"],"metadata":{"id":"mZOhZAQ8udyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Experimenting with Transpose convolution with tensorflow"],"metadata":{"id":"kELqsxvCYqd9"}},{"cell_type":"code","source":["import tensorflow as tf\n"],"metadata":{"id":"la_Cuz45Vgdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LJH58u2kNzHM","executionInfo":{"status":"ok","timestamp":1649164157411,"user_tz":-330,"elapsed":516,"user":{"displayName":"Animesh Sachan","userId":"04368887509161657070"}},"outputId":"44bd0b54-320c-4a6b-d295-70f0314d1ea0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[ 0.18330155 -0.5533184   0.22738452]\n","   [ 0.7073788  -0.4854805  -0.18231249]\n","   [-0.0832573  -0.05566709 -0.47303542]]\n","\n","  [[ 0.08941013 -0.40070945 -0.5542858 ]\n","   [ 0.22750475 -0.39118344 -0.41565126]\n","   [ 0.25596932 -0.40971956 -0.1890703 ]]\n","\n","  [[ 0.05023995 -0.06407139 -0.15725702]\n","   [-0.3692479  -0.09256556 -0.32696834]\n","   [ 0.3253673  -0.3716717   0.17068009]]]], shape=(1, 3, 3, 3), dtype=float32)\n"]}],"source":["kz = 2 # kernel_size\n","st = 1 # strides\n","\n","input = tf.random.uniform((1, 2, 2, 2)) #batch size, width, height, channels\n","\n","trans_conv1 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = kz, strides= st , padding='valid')\n","\n","output = trans_conv1(input)\n","\n","print(output)\n"]},{"cell_type":"code","source":["kz = 3 # kernel_size\n","st = 1 # strides\n","\n","input = tf.random.uniform((1, 2, 2, 2)) #batch size, width, height, channels\n","\n","trans_conv1 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = kz, strides= st , padding='valid')\n","\n","output = trans_conv1(input)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnUQVuY8XwmY","executionInfo":{"status":"ok","timestamp":1649164217493,"user_tz":-330,"elapsed":504,"user":{"displayName":"Animesh Sachan","userId":"04368887509161657070"}},"outputId":"f532edf5-22e2-4d2b-e7ed-83690b680f94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[-1.08339183e-01  3.15186717e-02  1.78843811e-01]\n","   [ 5.68277240e-02  1.62411064e-01  6.94189072e-02]\n","   [ 1.70952469e-01  2.63806432e-04  2.94180959e-01]\n","   [ 3.30078453e-02  8.03290494e-03  8.00928026e-02]]\n","\n","  [[ 4.76946235e-02 -2.31726453e-01  2.56205797e-01]\n","   [-2.77556658e-01 -1.52992055e-01  2.03182995e-01]\n","   [ 2.58328676e-01  1.91317558e-01  1.65269583e-01]\n","   [ 2.04924539e-01  6.48251176e-03  1.74412012e-01]]\n","\n","  [[ 1.16140537e-01 -3.86820316e-01  3.94807868e-02]\n","   [ 4.58863080e-02 -1.90832764e-01  7.97833130e-02]\n","   [ 1.58888936e-01 -1.30844653e-01 -3.30444276e-01]\n","   [ 6.86421692e-02  2.61069722e-02 -1.96500689e-01]]\n","\n","  [[-1.20005995e-01 -1.76531971e-01 -6.61680698e-02]\n","   [ 1.04666509e-01  5.34620732e-02 -8.39624256e-02]\n","   [ 3.54047775e-01 -1.57816976e-01 -9.18743908e-02]\n","   [ 1.59025311e-01 -2.01287925e-01 -2.54352335e-02]]]], shape=(1, 4, 4, 3), dtype=float32)\n"]}]},{"cell_type":"code","source":["kz = 3 # kernel_size\n","st = 2 # strides\n","\n","input = tf.random.uniform((1, 2, 2, 2)) #batch size, width, height, channels\n","\n","trans_conv1 = tf.keras.layers.Conv2DTranspose(filters = 3, kernel_size = kz, strides= st , padding='valid')\n","\n","output = trans_conv1(input)\n","\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6I64TYzYD1J","executionInfo":{"status":"ok","timestamp":1648904857445,"user_tz":-330,"elapsed":423,"user":{"displayName":"Team Spartificial","userId":"12223987377667674470"}},"outputId":"9b117fe6-8b24-46ab-e678-cca700ea9d22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[[-0.04421911 -0.06894263  0.04900293]\n","   [-0.07226346  0.04708602  0.01601166]\n","   [-0.12643951 -0.23452944  0.17883417]\n","   [-0.3032666   0.19758974  0.06748375]\n","   [ 0.25124922  0.23036814 -0.11326155]]\n","\n","  [[ 0.0310073  -0.09268826  0.00505753]\n","   [-0.01722443  0.02740151 -0.05210368]\n","   [ 0.0937487  -0.32524535  0.00128413]\n","   [-0.0727782   0.11478873 -0.21866834]\n","   [-0.15160584  0.266193   -0.08369448]]\n","\n","  [[-0.14748995 -0.23048097  0.12484887]\n","   [-0.26959863  0.15750004  0.04686622]\n","   [-0.09692953  0.2968594  -0.34635746]\n","   [ 0.0451934  -0.11400144  0.10167167]\n","   [-0.03372756 -0.07746136 -0.28200796]]\n","\n","  [[ 0.15572926 -0.41741964  0.01743059]\n","   [-0.00854215  0.14005695 -0.21540165]\n","   [-0.14558351  0.19590788 -0.0897291 ]\n","   [-0.03839193  0.02290654 -0.05892329]\n","   [-0.03401329  0.06334916 -0.01837292]]\n","\n","  [[-0.18158177  0.21949588 -0.2511167 ]\n","   [ 0.1378204  -0.22878046  0.02679643]\n","   [-0.13393018  0.0150971  -0.25914615]\n","   [ 0.03021458 -0.02557989  0.03282868]\n","   [-0.0416819  -0.0715844  -0.08176848]]]], shape=(1, 5, 5, 3), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["## Difference between Conv2DTranspose and UpSampling2D in keras\n","\n","Transpose: https://keras.io/api/layers/convolution_layers/convolution2d_transpose/\n","\n","UpSampling: https://keras.io/api/layers/reshaping_layers/up_sampling2d/"],"metadata":{"id":"XNUJwMkA6vMD"}},{"cell_type":"code","source":["# Reference: https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf"],"metadata":{"id":"e7Ne9N2UN_XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ut4aajgOU5uE"},"execution_count":null,"outputs":[]}]}