{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Advanced ML pipeline with segmentation_models and Callbacks (W7S1)","metadata":{"id":"kD6lbSNVLszk"}},{"cell_type":"markdown","source":"## **The most important thing to improve your model performance is to understand each and every step taken to build the final model.** \n\n*In this notebook, you will see how we increase our accuracy from 0.18 in previous lecture to 0.8 in this lecture.*\n","metadata":{"id":"CxcJ0zARmGOB"}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{"id":"pHL8t-2oLszp"}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport cv2\nimport keras\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport glob\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nimport math\nfrom tensorflow.keras.utils import to_categorical, Sequence\n\n\nimport datetime","metadata":{"id":"3eItJm1hLszr","execution":{"iopub.status.busy":"2022-05-30T07:01:55.754299Z","iopub.execute_input":"2022-05-30T07:01:55.754722Z","iopub.status.idle":"2022-05-30T07:02:03.316391Z","shell.execute_reply.started":"2022-05-30T07:01:55.754643Z","shell.execute_reply":"2022-05-30T07:02:03.315561Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"'''\nHere load_data function is called. This will load the dataset paths and \nsplit it into X_train, X_test, y_train, y_test '''\n\nimg_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\nmask_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n\n\n# let's get the list of image paths and mask paths in sorted order from the given directory respectively\nimages = [os.path.join(img_dir, x) for x in sorted(os.listdir(img_dir))]\nmasks = [os.path.join(mask_dir, x) for x in sorted(os.listdir(mask_dir))]\n\n\n# in this session, we will use our complete dataset\nX_train = images[:8000]\ny_train = masks[:8000]\n\nX_valid = images[8000:]\ny_valid = masks[8000:]","metadata":{"id":"ofv9tYqEkO5V","execution":{"iopub.status.busy":"2022-05-30T07:02:19.715658Z","iopub.execute_input":"2022-05-30T07:02:19.716309Z","iopub.status.idle":"2022-05-30T07:02:20.409514Z","shell.execute_reply.started":"2022-05-30T07:02:19.716275Z","shell.execute_reply":"2022-05-30T07:02:20.408678Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Building Dataset generator from scratch using Sequence","metadata":{"id":"OyRDdo0-kO5W"}},{"cell_type":"code","source":"# Here, `x_set` is list of path to the images\n# and `y_set` are the associated classes.\n# https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\nclass LunarDataset(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size, dims, classes):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.img_height, self.img_width = dims\n        self.classes = classes\n\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        count = 0\n        # https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\n        xtr = np.zeros((self.batch_size, self.img_height, self.img_width, 3))\n        for filename in batch_x:\n            img = imread(filename)[:self.img_height, :self.img_width, :] / 255.0\n            img = img.astype(np.float32)\n            xtr[count] = img\n            count += 1\n            \n        count = 0\n        ytr = np.zeros((self.batch_size, self.img_height, self.img_width, num_classes))\n        for filename in batch_y:\n            mask = imread(filename, as_gray = True)[:self.img_height, :self.img_width] // 0.07\n            mask[mask == 3] = 2\n            mask[mask == 10] = 3\n            \n            # one hot encoding our masks using to_categorical\n            # https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\n            mask = to_categorical(mask, num_classes = 4)\n            ytr[count] = mask\n            count += 1\n\n        return xtr, ytr.astype(np.int32)\n\n\nbatch_size = 16\ndims = (480, 480)\nnum_classes = 4\n\ntrain_dataset = LunarDataset(X_train, y_train, batch_size, dims, num_classes)\nvalid_dataset = LunarDataset(X_valid, y_valid, batch_size, dims, num_classes)","metadata":{"id":"8iibzosHkO5Y","execution":{"iopub.status.busy":"2022-05-30T07:02:23.883468Z","iopub.execute_input":"2022-05-30T07:02:23.883809Z","iopub.status.idle":"2022-05-30T07:02:23.897282Z","shell.execute_reply.started":"2022-05-30T07:02:23.883781Z","shell.execute_reply":"2022-05-30T07:02:23.895731Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Let's visualize our masks","metadata":{"id":"Kk5QMvOYkO5a"}},{"cell_type":"code","source":"sam = next(iter(train_dataset))\n\nsample = sam[1][1]\n\ni, v = np.unique(sample, return_counts = True)\nfor a,b in zip(i,v):\n    print(a,\" \", b)\n    \n\nfig, (a1, a2, a3, a4) = plt.subplots(1, 4, figsize = (20, 5))\n\na1.imshow(sample[:, :, 0])\na2.imshow(sample[:, :, 1])\na3.imshow(sample[:, :, 2])\na4.imshow(sample[:, :, 3])\n\nplt.show()\n\n# four channels showing different classes\n# each channel have only 0 and 1 values","metadata":{"id":"sz--yYp2kO5b","execution":{"iopub.status.busy":"2022-05-30T07:02:27.749602Z","iopub.execute_input":"2022-05-30T07:02:27.750357Z","iopub.status.idle":"2022-05-30T07:02:29.193657Z","shell.execute_reply.started":"2022-05-30T07:02:27.750323Z","shell.execute_reply":"2022-05-30T07:02:29.192113Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Let's check out some basic steps of transfer learning using a pretrained model (VGG16)\n","metadata":{"id":"lZ3xM6nTZUNX"}},{"cell_type":"code","source":"#### Step 1: Creating a base model \n\nIMG_SHAPE = (480, 480, 3)\n\n# include_top specify that we don't want to use the top layer (classifier)\nbase_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n\n\n\n\n#### Step 2: Freezing the base\n\n# It is important to freeze the convolutional base before you compile and train the model.\n# Freezing prevents the weights in a given layer from being updated during training\n# VGG16 has many layers, so setting the entire model's trainable flag to False will freeze all of them.\n\nbase_model.trainable = False\n\n# Let's take a look at the base model architecture\nbase_model.summary()\n\n\n\n#### Step 3: Adding the head\n\n# inputs\ninputs = tf.keras.Input(shape=(480, 480, 3))\n\n# base with pretrained model\nx = base_model(inputs, training=False)\n\n# head layers\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(2)(x)\n\n# model\nmodel = tf.keras.Model(inputs, outputs)\n\n# Let's take a look at the final model architecture\nmodel.summary()\n\n\n# reference: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html","metadata":{"id":"4V04gfvYZu-q","execution":{"iopub.status.busy":"2022-05-30T07:02:35.999058Z","iopub.execute_input":"2022-05-30T07:02:35.999862Z","iopub.status.idle":"2022-05-30T07:02:39.654744Z","shell.execute_reply.started":"2022-05-30T07:02:35.999824Z","shell.execute_reply":"2022-05-30T07:02:39.653899Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## segmentation_models","metadata":{"id":"bAqM5bf_s3QO"}},{"cell_type":"markdown","source":"\n**segmentation_models** is a python library with Neural Networks for Image Segmentation based on Keras and TensorFlow.\n\nThe main features of this library are:\n\n* High level API (just two lines of code to create model for segmentation)\n* 4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n* 25 available backbones for each architecture\n* All backbones have pre-trained weights for faster and better convergence\n* Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)","metadata":{"id":"khcnnP4asNE8"}},{"cell_type":"code","source":"# run this command to directly install the library in our notebook\n\n!pip install segmentation_models","metadata":{"id":"TQ481azQsf2A","execution":{"iopub.status.busy":"2022-05-30T07:02:56.514188Z","iopub.execute_input":"2022-05-30T07:02:56.515128Z","iopub.status.idle":"2022-05-30T07:03:08.795108Z","shell.execute_reply.started":"2022-05-30T07:02:56.515087Z","shell.execute_reply":"2022-05-30T07:03:08.793940Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"* Provide environment variable SM_FRAMEWORK=keras / SM_FRAMEWORK=tf.keras before import segmentation_models\n* Change framework sm.set_framework('keras') / sm.set_framework('tf.keras')","metadata":{"id":"apuK6SQMsbhs"}},{"cell_type":"code","source":"# By default it tries to import keras, if it is not installed, it will try to start with tensorflow.keras framework\n\nimport segmentation_models as sm\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nsm.set_framework('tf.keras')\ntf.keras.backend.set_image_data_format('channels_last')","metadata":{"id":"ckhpfcSNsXXG","execution":{"iopub.status.busy":"2022-05-30T07:03:23.838815Z","iopub.execute_input":"2022-05-30T07:03:23.839466Z","iopub.status.idle":"2022-05-30T07:03:23.864619Z","shell.execute_reply.started":"2022-05-30T07:03:23.839430Z","shell.execute_reply":"2022-05-30T07:03:23.863843Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Building our UNet model with segmentation_models","metadata":{"id":"Fdtf71zxkO5i"}},{"cell_type":"code","source":"BACKBONE = 'vgg16'\ninput_shape = (480, 480, 3)\nn_classes = 4\nactivation = 'softmax'\n\n# using segmentation_models to create U-net with vgg16 as a backbone\n# and pretrained imagenet weights\n\n# segmentation_model basically will create a mirror image of our backbone as expansion path and add to the contraction path\nmodel = sm.Unet(backbone_name = BACKBONE, \n                input_shape = input_shape, \n                classes = n_classes, \n                activation = activation,\n                encoder_weights = 'imagenet')\nmodel.summary()","metadata":{"id":"q9UGUpklLsz1","execution":{"iopub.status.busy":"2022-05-30T07:03:26.838280Z","iopub.execute_input":"2022-05-30T07:03:26.839173Z","iopub.status.idle":"2022-05-30T07:03:27.343329Z","shell.execute_reply.started":"2022-05-30T07:03:26.839121Z","shell.execute_reply":"2022-05-30T07:03:27.341433Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Compile model","metadata":{"id":"jFpUjTLxLsz2"}},{"cell_type":"code","source":"\"\"\" Hyperparameters \"\"\"\nlr = 1e-4\nbatch_size = 16\nepochs = 1\n\n# metrics for result validation\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n\n# compiling the model\nmodel.compile(loss = 'categorical_crossentropy', \n              optimizer = tf.keras.optimizers.Adam(lr), \n              metrics = metrics)\n\ntrain_steps = len(X_train)//batch_size\nvalid_steps = len(X_valid)//batch_size\n\n\n\"\"\" Callbacks \"\"\"\ncurrent_datetime = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks\ncallbacks = [\n        tf.keras.callbacks.ModelCheckpoint(filepath=f'models/lunarModel_{current_datetime}.h5',\n                        monitor='val_iou_score', verbose=0, \n                        mode='max', save_best_model=False),\n             \n        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_iou_score\", mode='max', patience=4,\n                          factor=0.1, verbose=0, min_lr=1e-6),\n             \n        tf.keras.callbacks.EarlyStopping(monitor=\"val_iou_score\", patience=5, verbose=0, mode='max'),\n\n        tf.keras.callbacks.TensorBoard(f'models/logs_{current_datetime}')\n    ]","metadata":{"id":"BAwylsJgLsz3","execution":{"iopub.status.busy":"2022-05-30T07:03:33.607327Z","iopub.execute_input":"2022-05-30T07:03:33.607686Z","iopub.status.idle":"2022-05-30T07:03:33.970893Z","shell.execute_reply.started":"2022-05-30T07:03:33.607655Z","shell.execute_reply":"2022-05-30T07:03:33.969114Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"\nmodel is compiled with **loss**=\"categorical_crossentropy\",  **optimizer**=Adam, **metrics**=iou_score\n\n**Callbacks** is a tool to customize the behavior of a Keras model during training, evaluation, or inference.\n\n**ModelCheckpoint:** used  to periodically save your model during training.\n\n**ReduceLROnPlateau:** Reduce learning rate when a metric has stopped improving.\n\n**EarlyStopping:** Stop training when a monitored metric has stopped improving.\n\n**TensorBoard:** Enable visualizations for TensorBoard.","metadata":{"id":"UhikbwebLsz3"}},{"cell_type":"markdown","source":"## Train model","metadata":{"id":"UPho7ynJLsz4"}},{"cell_type":"code","source":"# Fitting the model\nmodel_history = model.fit(train_dataset,\n        steps_per_epoch=train_steps,\n        validation_data=valid_dataset,\n        validation_steps=valid_steps,\n        epochs=epochs,\n        callbacks=callbacks\n    )\n# val_iou_score is expected to reach almost 0.8 after 5 epochs even without using callbacks","metadata":{"id":"eK6nMaYkLsz4","execution":{"iopub.status.busy":"2022-05-30T06:19:44.591294Z","iopub.execute_input":"2022-05-30T06:19:44.591679Z","iopub.status.idle":"2022-05-30T06:28:21.329890Z","shell.execute_reply.started":"2022-05-30T06:19:44.591648Z","shell.execute_reply":"2022-05-30T06:28:21.328987Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1/5\n500/500 [==============================] - 553s 1s/step - loss: 0.4765 - iou_score: 0.4762 - f1-score: 0.5403 - val_loss: 0.2147 - val_iou_score: 0.5423 - val_f1-score: 0.5970\nEpoch 2/5\n500/500 [==============================] - 508s 1s/step - loss: 0.1501 - iou_score: 0.6948 - f1-score: 0.7837 - val_loss: 0.1720 - val_iou_score: 0.6603 - val_f1-score: 0.7461\nEpoch 3/5\n500/500 [==============================] - 512s 1s/step - loss: 0.1204 - iou_score: 0.7489 - f1-score: 0.8338 - val_loss: 0.1116 - val_iou_score: 0.7383 - val_f1-score: 0.8242\nEpoch 4/5\n500/500 [==============================] - 508s 1s/step - loss: 0.1053 - iou_score: 0.7772 - f1-score: 0.8573 - val_loss: 0.1160 - val_iou_score: 0.7461 - val_f1-score: 0.8315\nEpoch 5/5\n500/500 [==============================] - 514s 1s/step - loss: 0.0989 - iou_score: 0.7877 - f1-score: 0.8656 - val_loss: 0.0906 - val_iou_score: 0.7976 - val_f1-score: 0.8726","metadata":{"id":"BA1oCaDikO5l"}},{"cell_type":"markdown","source":"## Predict from model","metadata":{"id":"IobGiGA6Lsz4"}},{"cell_type":"code","source":"# function to predict result \ndef predict_image(img_path, mask_path, model):\n    H = 480\n    W = 480\n    num_classes = 4\n\n    img = imread(img_path)\n    img = img[:480, :480, :]\n    img = img / 255.0\n    img = img.astype(np.float32)\n\n    ## Read mask\n    mask = imread(mask_path, as_gray = True)\n    mask = mask[:480, :480]\n    \n    ## Prediction\n    pred_mask = model.predict(np.expand_dims(img, axis=0))\n    pred_mask = np.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[0]\n    \n    \n    # calculating IOU score\n    inter = np.logical_and(mask, pred_mask)\n    union = np.logical_or(mask, pred_mask)\n    \n    iou = inter.sum() / union.sum()\n\n    return img, mask, pred_mask, iou","metadata":{"id":"W-1_LmkTLsz5","execution":{"iopub.status.busy":"2022-05-30T07:03:49.638313Z","iopub.execute_input":"2022-05-30T07:03:49.638692Z","iopub.status.idle":"2022-05-30T07:03:49.648953Z","shell.execute_reply.started":"2022-05-30T07:03:49.638660Z","shell.execute_reply":"2022-05-30T07:03:49.647909Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"img_path = '../input/artificial-lunar-rocky-landscape-dataset/images/render/render0042.png'\nmask_path = '../input/artificial-lunar-rocky-landscape-dataset/images/clean/clean0042.png'\n\nimg, mask, pred_mask, iou = predict_image(img_path, mask_path, model)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15, 10))\n\nax1.set_title(\"Input Image\")\nax1.imshow(img)\n\nax2.set_title(\"True Mask\")\nax2.imshow(mask, cmap = \"gray\")\n\nax3.set_title(\"Predicted mask with IOU score %.2f\"%(iou))\nax3.imshow(pred_mask, cmap = \"gray\")\n\nplt.show()","metadata":{"id":"314r8EbTLsz6","execution":{"iopub.status.busy":"2022-05-30T07:03:52.655890Z","iopub.execute_input":"2022-05-30T07:03:52.656285Z","iopub.status.idle":"2022-05-30T07:04:03.777600Z","shell.execute_reply.started":"2022-05-30T07:03:52.656256Z","shell.execute_reply":"2022-05-30T07:04:03.776040Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### You can see we have reached a satisfactory result for our project. ","metadata":{"id":"qgfn0B86kO5m"}},{"cell_type":"markdown","source":"## Can we load a saved model in a new session and use it for predictions - WITHOUT TRAINING AGAIN?\nYes, if we download it before shutting down our current kernel.\n\nOn starting the new session, we can upload the saved model and use it for initializing weights of a newly created model instance!\n\nThis is a new model 'variable' with optimized weights! - Kinda like our very own pretrained model :)","metadata":{}},{"cell_type":"code","source":"# create a new model instance\nnew_model = sm.Unet(backbone_name = BACKBONE, \n                input_shape = input_shape, \n                classes = n_classes, \n                activation = activation,\n                encoder_weights = 'imagenet')\n\n# define the path to the checkpointed model after uploading it\ncheckpoint_path = '../input/savedmodel1/lunarModel_20220530-061940.h5'\n\n#resolve dependencies - sometimes..\ndependencies = {\n    'iou_score': sm.metrics.IOUScore,\n    'f1-score': sm.metrics.FScore\n}\n\n# load the weights from the checkpoint\nnew_model = keras.models.load_model(checkpoint_path, custom_objects=dependencies)\n \n# use loaded info for prediction\nimg_path = '../input/artificial-lunar-rocky-landscape-dataset/images/render/render0028.png'\nmask_path = '../input/artificial-lunar-rocky-landscape-dataset/images/clean/clean0028.png'\n\nimg, mask, pred_mask, iou = predict_image(img_path, mask_path, new_model)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15, 10))\n\nax1.set_title(\"Input Image\")\nax1.imshow(img)\n\nax2.set_title(\"True Mask\")\nax2.imshow(mask, cmap = \"gray\")\n\nax3.set_title(\"Predicted mask with IOU score %.2f\"%(iou))\nax3.imshow(pred_mask, cmap = \"gray\")\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-30T07:04:21.255244Z","iopub.execute_input":"2022-05-30T07:04:21.255691Z","iopub.status.idle":"2022-05-30T07:04:26.986831Z","shell.execute_reply.started":"2022-05-30T07:04:21.255646Z","shell.execute_reply":"2022-05-30T07:04:26.986042Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## In next lecture, you will learn some best practices to optimize your models","metadata":{"id":"OaMfFmCXkO5m"}},{"cell_type":"code","source":"","metadata":{"id":"3ajAnaiZkO5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"q26M3dwpkO5n"},"execution_count":null,"outputs":[]}]}