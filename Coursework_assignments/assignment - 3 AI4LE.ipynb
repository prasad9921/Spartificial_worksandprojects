{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# <center> Assignment 3",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "###### IMPORTANT ########\nYour_Name = \" \"\nYour_Email_id = \" \"",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# import libraries \nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2 \nimport matplotlib.pyplot as plt",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-05-02T16:40:49.950511Z",
          "iopub.execute_input": "2022-05-02T16:40:49.950899Z",
          "iopub.status.idle": "2022-05-02T16:40:49.956239Z",
          "shell.execute_reply.started": "2022-05-02T16:40:49.950849Z",
          "shell.execute_reply": "2022-05-02T16:40:49.955171Z"
        },
        "trusted": true
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Q1 (3 points) \n#### Which of the following is not an activation function\n\n* a - Sigmoid function\n* b - Hyperbolic tangent function\n* c - Rectified linear unit (RELU) function\n* d - Leaky RELU function\n* e - dynamic function\n* f - Maxout function\n* g - gaussian function\n* h - sobel function\n* i - Exponential Linear unit (ELU) function",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# print the correct options here\nprint(\"h\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": "h\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Q2(3 points)\n\n#### Suppose a neuron N in layer 2 has 5 input neurons from layer 1. Do all these 5 input neurons have the same contribution towards the output of the neuron N? Explain.",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "# write your answer here \nprint(\"Definitely No. The reason is every neuron has different weights associated with it. The contribution of every neuron depends on the weights.\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": "Definitely No. The reason is every neuron has different weights associated with it. The contribution of every neuron depends on the weights.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Q3 (4 points)\n#### Organise below points in right sequence\n\n1. Dataset capturing \n2. Optimizing parameters with backpropagation\n3. Predicting the final output using our model\n4. Data preprocessing\n5. Calculating loss function\n6. Applying activation function on initial output\n7. Initializing weights and biases",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# print the right sequence here\nprint(\"1,4,7,6,5,2,3\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": "1,4,7,6,5,2,3\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Q4 (6 points) \n\n#### Define below points - \n\na. Why is normalization done in batches? \n\nb. When is it suitable to use batches? \n\nc. Can we do without batches? If yes, in which case?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# write you answers here\n\na = \"Batch normalization solves a major problem called internal covariate shift. It helps by making the data flowing between intermediate layers of the neural network look, this means you can use a higher learning rate. It has a regularizing effect which means you can often remove dropout.\"\nb = \"Batch processing is most often used when dealing with very large amounts of data, and/or when data sources are legacy systems that are not capable of delivering data in streams. Data generated on mainframes is a good example of data that.\"\nc = \"yes we can normalize without batches thorugh adaptation of training process. In the case of transfer learning\"\n\nprint(a)\nprint()\nprint(b)\nprint()\nprint(c)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": "Batch normalization solves a major problem called internal covariate shift. It helps by making the data flowing between intermediate layers of the neural network look, this means you can use a higher learning rate. It has a regularizing effect which means you can often remove dropout.\n\nBatch processing is most often used when dealing with very large amounts of data, and/or when data sources are legacy systems that are not capable of delivering data in streams. Data generated on mainframes is a good example of data that.\n\nyes we can normalize without batches thorugh adaptation of training process. In the case of transfer learning\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Q5  (4 points)\n\n#### Which propogation pass helps you (forward, backward)\n\na. obtain network output?\n\nb. update weights?\n\nc. calculate gradients?\n\nd. calculate loss?",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# print your answers below\n\nprint('forward')\n\nprint('backward')\n\nprint('backward')\n\nprint('forward')",
      "metadata": {
        "trusted": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": "forward\nbackward\nbackward\nforward\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Q6. (4 points)\n### Write categorical crossentropy loss in python from scratch and state a difference between categorial crossentropy and binary crossentropy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## write your answers here\nprint(\"Major difference between binary crossentropy and categorical crossentropy is that Binary deals with 2 classes whereas Categorial deals with multiple classes\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "text": "Major difference between binary crossentropy and categorical crossentropy is that Binary deals with 2 classes whereas Categorial deals with multiple classes\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Building a pokemon classification model using tensorflow",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Import dataset\n\n#### Go to Add data button and import https://www.kaggle.com/datasets/vishalsubbiah/pokemon-images-and-types dataset",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Q.7 (5 points)\n#### List all the image filenames present in \"../input/pokemon-images-and-types/images/images\" location and show first five images using matplotib subplots\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import os \nprint(os.getcwd())\nroot_dir = \"../input/pokemon-images-and-types/images/images\"\n\nfiles = os.listdir(root_dir)\nprint(files)\n\n#plot\nimgs=[]\nfor i in range(5):\n    img = cv2.imread(\"../input/pokemon-images-and-types/images/images/\"+files[i])\n    imgs.append(img)\n_, axs = plt.subplots(1, 5, figsize=(12, 12))\naxs = axs.flatten()\nfor img, ax in zip(imgs, axs):\n    ax.imshow(img)\nplt.show()\nfig=plt.figure()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": "/home/pyodide\n",
          "output_type": "stream"
        },
        {
          "ename": "<class 'FileNotFoundError'>",
          "evalue": "[Errno 44] No such file or directory: 'notebooks/input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      4\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks/input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(files)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 44] No such file or directory: 'notebooks/input'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Run the below 3 cells as it is",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Run the below cells as it is\ndata = pd.read_csv(\"../input/pokemon-images-and-types/pokemon.csv\")\n\ndata.head()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## We are going to use Type1 column as our labels. Each Name is unique and classified into 18 Type1 types. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Run the below cells as it is\ndata_dict = {}\n\nfor key, val in zip(data[\"Name\"], data[\"Type1\"]):\n    data_dict[key] = val\nprint(data_dict)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Run the below cells as it is\nlabels = data[\"Type1\"].unique()\nprint(labels)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Q8. (7 points)\n\n#### Create a dictionary and assign each label in labels list a unique id from 1 to 18. Name the dictionary as \"labels_idx\"",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## write your code here\n\nlabels_idx = {}\nfor i in range(len(labels)):\n    labels_idx[labels[i]]=i+1\nlabels_dict=labels_idx\nprint(labels_dict)",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Q.9 (4 points)\n#### Understand and Complete the below code",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "final_images = []\nfinal_labels = []\n# count = 0\nfor file in files:\n    # pass an argument in place of \"arg\" in below line so that we can read image as grayscale\n    img = cv2.imread(os.path.join(root_dir, file), 0) \n    label = labels_idx[data_dict[file.split(\".\")[0]]] # read this line atleast four times for better understanding\n    \n    # append img in final_images list\n    final_images.append(img)\n    # append label in final_labels list\n    final_labels.append(label)\n    \n    \n# converting lists into numpy arrayn\n# normalizing and reshaping the data (do not make any change)\nfinal_images = np.array(final_images, dtype = np.float32)/255.0\nfinal_labels = np.array(final_labels, dtype = np.int8).reshape(809, 1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### We have segregated our data into images and labels and is the time to build our model using tensorflow",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Q.10 (10 points)\n\n#### Complete the following code to create a 1 input, 3 layer fully connected and an output layer network to provide final output of 18 classes. \n\nNOTE: Use suitable activation functions, number of neurons and try to keep the trainbale parameters below 1 million",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\n\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(120, 120,3)),\n    tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(100, activation='relu'),\n    tf.keras.layers.Dense(18)\n])\n# print model summary and check trainable parameters\nmodel.summary()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# compile model (Use: Adam optimizer, categorical_crossentropy loss and metrics as Accuracy)\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n''' Fit model for training for 50 epochs (make sure to turn your notebook's GPU Accelerator on from settings given on the right hand side)\nIf you do not see Accelarator and Internet options in Setting then please verify your kaggle profile using your phone number '''\n# fit model (use images and labels)\n\nhistory = model.fit(final_images, final_labels, epochs=50)\n\n# predict the class of any image using our trained model\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# predict the class of any image using our trained model\n\nprobability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\npredictions = probability_model.predict(final_images)\n\nprint(\"\\n\",predictions[0])\nid = np.argmax(predictions[0])\nprint(\"\\nid that we got from the model as prediction: {}\\nType of pokemon associted with that id: {} \".format(id,labels[id]))\nprint(\"accuracy of the model\",history.history['accuracy'][-1])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}