{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xksEYwr8QXgN"
   },
   "source": [
    "# <center>Assignment 6 [Final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T11:01:47.531883Z",
     "iopub.status.busy": "2022-02-15T11:01:47.531504Z",
     "iopub.status.idle": "2022-02-15T11:01:47.537092Z",
     "shell.execute_reply": "2022-02-15T11:01:47.535364Z",
     "shell.execute_reply.started": "2022-02-15T11:01:47.531851Z"
    },
    "id": "2WVqrOVwm8yz"
   },
   "outputs": [],
   "source": [
    "Your_name = \"\"\n",
    "Your_emailid = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qWKqzF4415S"
   },
   "source": [
    "### **NOTE: Open this notebook in kaggle and import Artificial Lunar Landscape Dataset.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb3HM3Asm8y1"
   },
   "source": [
    "## > If you run this notebook as it is, you will get the val_iou_score of around 0.20 (remember to use GPU for training the model)\n",
    "\n",
    "## > Your goal is to increase the val_iou_score as much as you can for this project using any method. The evaluation of this assignment will be based on your acquired val_iou_score. One point for each increasing 0.01 val_iou_score. \n",
    "\n",
    "> For example - if val_iou_score = 0.41, your points will be 41/100. \n",
    "\n",
    "## > Please check your notebook before submission, try to avoid any error.\n",
    "\n",
    "### Some tips to increase the performance\n",
    "* Increase the number of epochs\n",
    "* Increase the number of layers in your model\n",
    "* Using SOTA high performance networks with transfer learning\n",
    "* Using callbacks and carefully observing your model performance\n",
    "\n",
    "You are free to use other techniques too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSjKJ2JQtLV9"
   },
   "source": [
    "# GUIDELINES ABOUT MAKING CHANGES TO THIS NOTEBOOK\n",
    "\n",
    "For every change you make to this notebook, only those supported by the following would be considered for evaluation:\n",
    "\n",
    "1. A descriptive comment explaining the change, e.g., if you are adding an extra conv2d layer, write about all the aspects of the conv2d layer you are adding. The comment should be placed at the point where the layer will be added. \n",
    "\n",
    "2. Changes brought to the system because of changes you introduced, e.g., if you changed layers - added, deleted, etc., you MUST show model properties before and after the changes were made.\n",
    "\n",
    "3. Data preprocessing changes - if you use new data preprocessing techniques that are not a part of this notebook, you MUST explain their inner workings using 2-3 MARKDOWN cells, and ONLY AFTER THAT,  proceed to use that technique. Without this explanation, your technique will not be considered for evaluation.\n",
    "\n",
    "4. ALL improvements MUST BE REPORTED VIA PLOTS OR TABLES OR BOTH, e.g., if increasing epochs from 30 to 50 improved your results, but decreasing learning_rate from 0.0001 to 0.00005 also improved your results, then these gains FIRST HAVE TO BE REPORTED SEPARATELY VIA PLOTS, THEN AGAIN TOGETHER VIA TABLES. \n",
    "\n",
    "  One plot can show iou values increasing from epoch 30 to epoch 50. Another plot can show iou values improving at varying learning rates. Finally tables can be used to show iou values for different learning rates, table 1 for lr_1 shows iou for epochs 30 through 50, table 2 for lr_2 shows iou for epochs 30 through 50, and so on and so forth - ALL YOUR RESULTS MUST BE COMPULSORILY QUANTIFIABLE! \n",
    "\n",
    "It is therefore advised to work on one improvement, optimize it, plot it, document it, then proceed to the next improvement - till you get a satisfactory IOU score.\n",
    "\n",
    "5. FINAL IMPROVEMENT SUMMARY TABLE: Prepare a table with columns (improv#, description, increase in iou from, increase in iou to), and list out all the improvements you made to improve your model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:28.584751Z",
     "iopub.status.busy": "2022-02-15T10:44:28.584014Z",
     "iopub.status.idle": "2022-02-15T10:44:40.914565Z",
     "shell.execute_reply": "2022-02-15T10:44:40.913494Z",
     "shell.execute_reply.started": "2022-02-15T10:44:28.584657Z"
    },
    "id": "HRJUKjN1QXgS"
   },
   "outputs": [],
   "source": [
    "!pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:40.917677Z",
     "iopub.status.busy": "2022-02-15T10:44:40.917309Z",
     "iopub.status.idle": "2022-02-15T10:44:48.094217Z",
     "shell.execute_reply": "2022-02-15T10:44:48.093288Z",
     "shell.execute_reply.started": "2022-02-15T10:44:40.917631Z"
    },
    "id": "bGIcq3_DQXgT"
   },
   "outputs": [],
   "source": [
    "# import the necessary Library\n",
    "\n",
    "import tensorflow as tf\n",
    "import segmentation_models as sm\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import keras \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIJMgWvKm8y7"
   },
   "source": [
    "* Provide environment variable SM_FRAMEWORK=keras / SM_FRAMEWORK=tf.keras before import segmentation_models\n",
    "* Change framework sm.set_framework('keras') / sm.set_framework('tf.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2HKIVofm8y7"
   },
   "outputs": [],
   "source": [
    "# Setting framework environment\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "sm.set_framework('tf.keras')\n",
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdS9NTtwQXgV"
   },
   "source": [
    "## Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:48.460018Z",
     "iopub.status.busy": "2022-02-15T10:44:48.459695Z",
     "iopub.status.idle": "2022-02-15T10:44:48.480887Z",
     "shell.execute_reply": "2022-02-15T10:44:48.479853Z",
     "shell.execute_reply.started": "2022-02-15T10:44:48.459962Z"
    },
    "id": "EQGsLbOVQXgW"
   },
   "outputs": [],
   "source": [
    "H = 256 # height of image\n",
    "W = 256 # width of image\n",
    "\n",
    "'''This function is used to return the list of path for images and masks in\n",
    "sorted order from the given directory respectively.'''\n",
    "# function to return list of image paths and mask paths \n",
    "def process_data(IMG_DIR, MASK_DIR):\n",
    "    images = [os.path.join(IMG_DIR, x) for x in sorted(os.listdir(IMG_DIR))]\n",
    "    masks = [os.path.join(MASK_DIR, x) for x in sorted(os.listdir(MASK_DIR))]\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "'''This function is used to return splitted list of images and corresponding \n",
    "mask paths in train and test by providing test size.'''\n",
    "# function to load data and train test split\n",
    "def load_data(IMG_DIR, MASK_DIR):\n",
    "    X, y = process_data(IMG_DIR, MASK_DIR)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "'''This function is used to read images. It takes image path as input. \n",
    "After reading image it is resized by width and height provide above(256 x 256). \n",
    "Next normalization is done by dividing each values with 255. And the result is returned.'''\n",
    "# function to read image\n",
    "def read_image(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x / 255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "'''This function is used to read masks.'''\n",
    "# function to read mask\n",
    "def read_mask(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x.astype(np.int32)\n",
    "    return x\n",
    "\n",
    "'''This function is used to generate tensorflow data pipeline. \n",
    "The tensorflow data pipeline is mapped to function ‘preprocess’ .'''\n",
    "# function for tensorflow dataset pipeline\n",
    "def tf_dataset(x, y, batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset\n",
    "\n",
    "'''This function takes image and mask path. \n",
    "It reads the image and mask as provided by paths. \n",
    "Mask is one hot encoded for multi class segmentation (here 4 class).'''\n",
    "# function to read image and mask amd create one hot encoding for mask\n",
    "def preprocess(x, y):\n",
    "    def f(x, y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "\n",
    "        image = read_image(x)\n",
    "        mask = read_mask(y)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n",
    "    mask = tf.one_hot(mask, 4, dtype=tf.int32)\n",
    "    image.set_shape([H, W, 3])\n",
    "    mask.set_shape([H, W, 4])\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufOlyg7MQXgY"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:48.484174Z",
     "iopub.status.busy": "2022-02-15T10:44:48.483759Z",
     "iopub.status.idle": "2022-02-15T10:44:49.277184Z",
     "shell.execute_reply": "2022-02-15T10:44:49.276146Z",
     "shell.execute_reply.started": "2022-02-15T10:44:48.484116Z"
    },
    "id": "vHWstFNTQXgY"
   },
   "outputs": [],
   "source": [
    "'''RENDER_IMAGE_DIR_PATH: ‘Path of image directory’\n",
    "GROUND_MASK_DIR_PATH: ‘Path of mask directory’\n",
    "\n",
    "Here load_data function is called. This will load the dataset paths and \n",
    "split it into X_train, X_test, y_train, y_test '''\n",
    "\n",
    "RENDER_IMAGE_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\n",
    "GROUND_MASK_DIR_PATH = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(RENDER_IMAGE_DIR_PATH, GROUND_MASK_DIR_PATH)\n",
    "print(f\"Dataset:\\n Train: {len(X_train)} \\n Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfSTVsjCQXgZ"
   },
   "source": [
    "## Generate tensorflow data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:49.278799Z",
     "iopub.status.busy": "2022-02-15T10:44:49.278505Z",
     "iopub.status.idle": "2022-02-15T10:44:52.379956Z",
     "shell.execute_reply": "2022-02-15T10:44:52.378982Z",
     "shell.execute_reply.started": "2022-02-15T10:44:49.278758Z"
    },
    "id": "4xsJKtW0QXgZ"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "'''Here the tf_dataset function is called will generate the tensorflow data pipeline.'''\n",
    "# calling tf_dataset\n",
    "train_dataset = tf_dataset(X_train, y_train, batch=batch_size)\n",
    "valid_dataset = tf_dataset(X_test, y_test, batch=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MqxtDTmQXga"
   },
   "source": [
    "## Creating U-net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:52.382189Z",
     "iopub.status.busy": "2022-02-15T10:44:52.381670Z",
     "iopub.status.idle": "2022-02-15T10:44:52.400638Z",
     "shell.execute_reply": "2022-02-15T10:44:52.399468Z",
     "shell.execute_reply.started": "2022-02-15T10:44:52.382127Z"
    },
    "id": "tYCyf8smQXga"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "'''conv_block it is used to create one block with two convolution layer \n",
    "followed by BatchNormalization and activation function relu. \n",
    "If the pooling is required then Maxpool2D is applied and return it else not.'''\n",
    "# function to create convolution block\n",
    "def conv_block(inputs, filters, pool=True):\n",
    "    x = Conv2D(filters, 3, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    if pool == True:\n",
    "        p = MaxPool2D((2, 2))(x)\n",
    "        return x, p\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "'''build_unet it is used to create the U-net architecture.'''\n",
    "# function to build U-net\n",
    "def build_unet(shape, num_classes):\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    \"\"\" Encoder \"\"\"\n",
    "    x1, p1 = conv_block(inputs, 16, pool=True)\n",
    "    x2, p2 = conv_block(p1, 32, pool=True)\n",
    "    x3, p3 = conv_block(p2, 48, pool=True)\n",
    "    x4, p4 = conv_block(p3, 64, pool=True)\n",
    "\n",
    "    \"\"\" Bridge \"\"\"\n",
    "    b1 = conv_block(p4, 128, pool=False)\n",
    "\n",
    "    \"\"\" Decoder \"\"\"\n",
    "    u1 = UpSampling2D((2, 2), interpolation=\"bilinear\")(b1)\n",
    "    c1 = Concatenate()([u1, x4])\n",
    "    x5 = conv_block(c1, 64, pool=False)\n",
    "\n",
    "    u2 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x5)\n",
    "    c2 = Concatenate()([u2, x3])\n",
    "    x6 = conv_block(c2, 48, pool=False)\n",
    "\n",
    "    u3 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x6)\n",
    "    c3 = Concatenate()([u3, x2])\n",
    "    x7 = conv_block(c3, 32, pool=False)\n",
    "\n",
    "    u4 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x7)\n",
    "    c4 = Concatenate()([u4, x1])\n",
    "    x8 = conv_block(c4, 16, pool=False)\n",
    "\n",
    "    \"\"\" Output layer \"\"\"\n",
    "    output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x8)\n",
    "\n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:52.402442Z",
     "iopub.status.busy": "2022-02-15T10:44:52.402021Z",
     "iopub.status.idle": "2022-02-15T10:44:52.938899Z",
     "shell.execute_reply": "2022-02-15T10:44:52.938020Z",
     "shell.execute_reply.started": "2022-02-15T10:44:52.402377Z"
    },
    "id": "65hPnreJQXgb"
   },
   "outputs": [],
   "source": [
    "# calling build_unet function\n",
    "model = build_unet((256, 256, 3), 4)\n",
    "\n",
    "#printing model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMgeqmX2QXgc"
   },
   "source": [
    "## Load model and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:52.942695Z",
     "iopub.status.busy": "2022-02-15T10:44:52.942423Z",
     "iopub.status.idle": "2022-02-15T10:44:53.371627Z",
     "shell.execute_reply": "2022-02-15T10:44:53.370488Z",
     "shell.execute_reply.started": "2022-02-15T10:44:52.942664Z"
    },
    "id": "z91qV2ZwQXgc"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from segmentation_models.metrics import iou_score\n",
    "import datetime, os\n",
    "\n",
    "\"\"\" Defining Hyperparameters \"\"\"\n",
    "img_shape = (256, 256, 3)\n",
    "num_classes = 4\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "\n",
    "\"\"\" Model building and compiling \"\"\"\n",
    "model = build_unet(img_shape, num_classes)\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=tf.keras.optimizers.Adam(lr), \n",
    "              metrics=[iou_score])\n",
    "\n",
    "\n",
    "train_steps = len(X_train)//batch_size\n",
    "valid_steps = len(X_test)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBhRPBKPQXgc"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T10:44:53.375475Z",
     "iopub.status.busy": "2022-02-15T10:44:53.374921Z",
     "iopub.status.idle": "2022-02-15T10:54:55.836226Z",
     "shell.execute_reply": "2022-02-15T10:54:55.835268Z",
     "shell.execute_reply.started": "2022-02-15T10:44:53.375429Z"
    },
    "id": "4lJgBNVwQXgd"
   },
   "outputs": [],
   "source": [
    "'''model.fit is used to train the model'''\n",
    "model_history = model.fit(train_dataset,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_data=valid_dataset,\n",
    "        validation_steps=valid_steps,\n",
    "        epochs=epochs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1OFxBbnxC_I"
   },
   "source": [
    "## [IMPORTANT] Paste you final model training history here in the markdown.(just double click this line, and you'll be able to edit it. \n",
    "\n",
    "NOTE: If we find that your actual model score and what you paste here is differing, your assignment will get rejected.  \n",
    "\n",
    "here ----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8f68O0lxXuP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment-6-final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
