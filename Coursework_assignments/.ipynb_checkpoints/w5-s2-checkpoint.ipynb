{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHg1npa6VNWa"
   },
   "source": [
    "# <center> W5S2 - Data Annotation using Labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DgBFjQMAW5W"
   },
   "source": [
    "## Topic already Covered:\n",
    "1. Computer Vision Introduction\n",
    "2. Convolution Operation\n",
    "3. Edge Detection\n",
    "4. Pooling operations, Padding\n",
    "5. State of the Art CNNs\n",
    "6. Data Augmentations\n",
    "7. Image Classification using tensorflow\n",
    "\n",
    "## In this session you will learn - \n",
    "1. Topic Already Covered:\n",
    "2. Data Exploration\n",
    "3. Labelme Installation\n",
    "4. Data Annotation\n",
    "5. Annotation to mask Conversion\n",
    "6. IOU and related metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Roi7rybo4kGE"
   },
   "source": [
    "### Our target for this project is to build an autonomous Vision system that can accurately detect the obstacles in front of the rover its set up on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOhV94iO4kGE"
   },
   "source": [
    "## Let's understand our dataset that we are going to use to build our Vision System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcjadj1y4kGE"
   },
   "source": [
    "# Background and motivation\n",
    "As we all know, it is always difficult to find good datasets for image analysis, even more so when the data of interest is difficult to collect. In the field of space robotics, firsthand data is very scarce and seldom freely accessible. To the knowledge of the authors, there exists no labelled dataset of planetary landscape images that could be used for any kind of machine learning approach to object detection or segmentation. Our goal is to openly provide a decent substitute for anyone who wishes to use such an approach on a lunar environment without manual labelling.\n",
    "\n",
    "# First Look at the Data\n",
    "Below is a side-by-side view of the first image of the dataset (render0001.tif) and its corresponding segmented ground truth (ground0001.tif).\n",
    "\n",
    "![Render and its ground truth segmentation](https://i.imgur.com/m5ja0BI.png)\n",
    "\n",
    "From this, we can make a number of observations that should be kept in mind when using this dataset, in decreasing order of importance:\n",
    "* **3 **(technically 4)** classes are considered: large rocks (in blue), smaller rocks (in green), the sky (in red)** (and everything else, in black)\n",
    "* **The segmented ground truth is not perfect**. In particular, there are some instances (like above) where a small rock is embedded in a larger rock. We recommend all users of the dataset to be aware of this when using the segmentation data, and we provide cleaned-up images of the segmentation under the images/clean folder. For users wishing to clean up images differently also show an example of how to morphologically close holes inside large rocks, apply thresholds and removes smaller rocks in the code snips below.\n",
    "* **The camera used is noise-free, and no data augmentation is performed**. It is up to the user to decide on what data augmentation pipeline they see fit to use, though we recommend that particular attention be paid to **adding sensor noise to renders, especially if the goal is to perform validation test on real lunar images**. An example of such sensor effect augmentation can be found here: https://github.com/alexacarlson/SensorEffectAugmentation.\n",
    "* **Only rocks measuring more than 10cm are usually represented on the segmented image**. This is to avoid cluttering and focus on rocks that are relevant to detect;\n",
    "* **Colors go darker for distant rocks**. This is also to keep focus on relevant rocks, which are usually closest to the observer. However, the user is free to set their own threshold to determine whether or not a certain rock should be considered depending on its color intensity (cf. the blue rock). As can be seen from the images below, giving a minimum intensity threshold betweeen 50 and 200 is recommended to avoid noise from distant rocks.\n",
    "* **Bounding boxes are only drawn around blue rocks of intensity above 150 and dimensions above 20x20 pixels**. This is, again, to only consider rocks that are clearly visible while leaving aside those further away. \n",
    "\n",
    "\n",
    "# Testing on real lunar pictures\n",
    "You may have noticed that real lunar pictures are also provided alongside the renders. Those were taken by the Chang'e 3 rover (image credit: CNSA), equipped with two cameras: PCAM and TCAM. Those cameras are quite different and we found it useful to keep their images separate so users can identify how well their segmentation works on each camera. We also hand-drew ground truth segmentation for these  images, arbitrarily selecting blue and green rocks (keep in mind that not all rocks are present in the ground truth when examining results such as false postitives). As an example, we tried using a modified UNet architecture on the dataset and used the trained result on some of Chang'e 3's images. Below are examples of both the potential and the limitations of the dataset. While some rocks are correctly and precisely detected, others (such as rocks with complex shapes, or those partially covered in sands) are not detected at all. We hope that this dataset put in the right hands will lead to even better results.\n",
    "\n",
    "![Test on real lunar pictures](https://i.imgur.com/7GRUlcS.png)\n",
    "(from left to right: original picture, ground truth (hand-drawn), segmentation trained with the Artificial Lunar Landscape Dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOe-UhyVVK8E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5plMhYPl4kGG"
   },
   "source": [
    "## Using Labelme to annotate our images \n",
    "\n",
    "First let's install the labelme tool\n",
    "\n",
    "1. go to https://pypi.org/project/labelme/\n",
    "2. choose your system, and install accordingly\n",
    "3. after installation, you can directly type \"labelme\" in your terminal to open labelme\n",
    "\n",
    "If you have python already installed in your system, then labelme can be directly install using \"pip install labelme\"\n",
    "\n",
    "#### You can use any data to try labelme. We recommend you to download this small subset of data that we are using for this project only. \n",
    "#### https://drive.google.com/drive/folders/1Gn3F_1Kue0yknwXpyzkFr2GkbmZfytLU?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHWPJq8y4kGG"
   },
   "source": [
    "## Follow along with the lecture for more understanding of data annotation using labelme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqG8USfZ4kGH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhL4xwJ74kGH"
   },
   "source": [
    "## IOU Metrics\n",
    "### Intersection over Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWpZ7nH4kGH"
   },
   "source": [
    "\n",
    ">The Intersection over Union (IoU) metric, also referred to as the Jaccard index, is essentially a method to quantify the percent overlap between the target mask and our prediction output. This metric is closely related to the Dice coefficient which is often used as a loss function during training.\n",
    "\n",
    "Quite simply, the IoU metric measures the number of pixels common between the target and prediction masks divided by the total number of pixels present across both masks.\n",
    "\n",
    "\n",
    "## $$ IoU = \\frac{{target \\cap prediction}}{{target \\cup prediction}} $$\n",
    "\n",
    "\n",
    "As a visual example, let's suppose we're tasked with calculating the IoU score of the following prediction, given the ground truth labeled mask.\n",
    "\n",
    "![](https://www.jeremyjordan.me/content/images/2018/05/target_prediction.png)\n",
    "\n",
    "\n",
    "The intersection (A∩B) is comprised of the pixels found in both the prediction mask and the ground truth mask, whereas the union (A∪B) is simply comprised of all pixels found in either the prediction or target mask.\n",
    "\n",
    "![](https://www.jeremyjordan.me/content/images/2018/05/intersection_union.png)\n",
    "\n",
    "\n",
    "The IoU score is calculated for each class separately and then averaged over all classes to provide a global, mean IoU score of our semantic segmentation prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-22T12:09:11.81416Z",
     "iopub.status.busy": "2022-03-22T12:09:11.81322Z",
     "iopub.status.idle": "2022-03-22T12:09:11.958474Z",
     "shell.execute_reply": "2022-03-22T12:09:11.957313Z",
     "shell.execute_reply.started": "2022-03-22T12:09:11.813993Z"
    },
    "id": "Zg8Q6Hl44kGH"
   },
   "outputs": [],
   "source": [
    "#### We can calculate this easily using Numpy.\n",
    "\n",
    "intersection = np.logical_and(target, prediction)\n",
    "\n",
    "union = np.logical_or(target, prediction)\n",
    "\n",
    "iou_score = np.sum(intersection) / np.sum(union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsn8x4YZ4kGI"
   },
   "source": [
    "### In next session you will learn Image processing and filtering using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDvJHTV14kGI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XG9Z14KnZutH"
   },
   "source": [
    "https://www.kaggle.com/code/brsdincer/lunar-landscape-segmentation-all-process-ae-ai/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
