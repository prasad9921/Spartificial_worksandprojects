{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elcpXOswLRgc"
   },
   "source": [
    "# <center> Assignment 5 (AI4LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:02:33.434253Z",
     "iopub.status.busy": "2022-05-24T12:02:33.432555Z",
     "iopub.status.idle": "2022-05-24T12:02:33.461887Z",
     "shell.execute_reply": "2022-05-24T12:02:33.461197Z",
     "shell.execute_reply.started": "2022-05-24T12:02:33.432776Z"
    },
    "id": "cfmCfF7eiTpk"
   },
   "outputs": [],
   "source": [
    "Your_Name = \"Selvaganapathy Senthamaraikannan \"\n",
    "Your_Email_id = \"selva95ganapathy@gmail.com \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juQiIIMl4qVz"
   },
   "source": [
    "## Open this notebook in kaggle for easy access to the dataset and related libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:02:36.884257Z",
     "iopub.status.busy": "2022-05-24T12:02:36.883395Z",
     "iopub.status.idle": "2022-05-24T12:02:44.03849Z",
     "shell.execute_reply": "2022-05-24T12:02:44.037557Z",
     "shell.execute_reply.started": "2022-05-24T12:02:36.88421Z"
    },
    "id": "f5ayJkkRwkjb"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d72b-W8fFaKI"
   },
   "source": [
    "## Data \n",
    "\n",
    "To get the dataset for this assignment, got to Add data option and search for https://www.kaggle.com/datasets/datatangai/people-with-occlusion-and-multipose-face-data in Search URL field.\n",
    "\n",
    "Import this dataset and there is only one image in this dataset. We are going to use this image only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:02:52.912217Z",
     "iopub.status.busy": "2022-05-24T12:02:52.911941Z",
     "iopub.status.idle": "2022-05-24T12:02:54.331847Z",
     "shell.execute_reply": "2022-05-24T12:02:54.330416Z",
     "shell.execute_reply.started": "2022-05-24T12:02:52.912188Z"
    },
    "id": "T808--eV4qV0"
   },
   "outputs": [],
   "source": [
    "img_path = \"../input/people-with-occlusion-and-multipose-face-data/3.png\"\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAhrBKqmjuHB"
   },
   "source": [
    "## Q1 (1 Point) \n",
    "\n",
    "Convert the images \"img\" into an array and store that array into 'img_arr' variable.\n",
    "\n",
    "Plot the img_arr using plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:03:02.447636Z",
     "iopub.status.busy": "2022-05-24T12:03:02.44725Z",
     "iopub.status.idle": "2022-05-24T12:03:02.475684Z",
     "shell.execute_reply": "2022-05-24T12:03:02.474923Z",
     "shell.execute_reply.started": "2022-05-24T12:03:02.447587Z"
    },
    "id": "ShSSOgg350kW"
   },
   "outputs": [],
   "source": [
    "# Complete the following code\n",
    "\n",
    "\n",
    "img_arr = np.array(img)\n",
    "img_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:03:06.198064Z",
     "iopub.status.busy": "2022-05-24T12:03:06.197585Z",
     "iopub.status.idle": "2022-05-24T12:03:06.973793Z",
     "shell.execute_reply": "2022-05-24T12:03:06.972643Z",
     "shell.execute_reply.started": "2022-05-24T12:03:06.198029Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot img_arr\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.imshow(img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqNymgaxpk1i"
   },
   "source": [
    "## Q2 (2 Points)\n",
    "Run the below code.\n",
    "\n",
    "Do you see anything abnormal with the shape of the above image? If yes, state the reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:03:15.971999Z",
     "iopub.status.busy": "2022-05-24T12:03:15.971702Z",
     "iopub.status.idle": "2022-05-24T12:03:15.97995Z",
     "shell.execute_reply": "2022-05-24T12:03:15.978838Z",
     "shell.execute_reply.started": "2022-05-24T12:03:15.971969Z"
    },
    "id": "23zzRR39pj-X"
   },
   "outputs": [],
   "source": [
    "print(\"Image shape:\", img_arr.shape)\n",
    "\n",
    "## print your reason here\n",
    "\n",
    "reason = ''' \n",
    "Yes,the y-axis values is decreasing from bottom up.\n",
    "\n",
    "Reason: \n",
    "The default behavior of imshow is to put the origin of the coordinate system in the \n",
    "upper left corner. This is different from plotting scientific data, such as two entities\n",
    "x and y against each other, where the origin, i.e. the point corresponding to the \n",
    "coordinate (0,0), is in the lower left corner, with the (positive) x-axis extending\n",
    "to the right and the (positive) y-axis extending towards the top.\n",
    "\n",
    "'''\n",
    "\n",
    "print(reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wj839JEq2vN"
   },
   "source": [
    "## Q3 (6 Points)\n",
    "The above image \"img_arr\" is a combination of ten different images, separate all ten images using numpy and show all of them using plt.subplot.\n",
    "\n",
    "(the size and shape of individual images should remain the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:03:37.755751Z",
     "iopub.status.busy": "2022-05-24T12:03:37.755336Z",
     "iopub.status.idle": "2022-05-24T12:03:39.112936Z",
     "shell.execute_reply": "2022-05-24T12:03:39.11202Z",
     "shell.execute_reply.started": "2022-05-24T12:03:37.755719Z"
    }
   },
   "outputs": [],
   "source": [
    "# first we are converting our four channels image into three channels image (easy to use and no extra complexities)\n",
    "'''Do not make any change in this below line of code.'''\n",
    "img_arr = img_arr[:, :, :3]\n",
    "\n",
    "#Getting No of row and column values\n",
    "row,col = img_arr.shape[0],img_arr.shape[1]\n",
    "#Vertically dividing the image into parts  \n",
    "Vhalf = row//2\n",
    "#Splitting into 5 parts of image\n",
    "Split = col//5\n",
    "\n",
    "#Using Numpy array value I am splitting the above image in 10 different parts\n",
    "top_img1 = img_arr[:Vhalf, :Split, :3]\n",
    "top_img2 = img_arr[:Vhalf, Split:Split*2, :3]\n",
    "top_img3 = img_arr[:Vhalf, Split*2:Split*3, :3]\n",
    "top_img4 = img_arr[:Vhalf, Split*3:Split*4, :3]\n",
    "top_img5 = img_arr[:Vhalf, Split*4:Split*5, :3]\n",
    "bot_img6 = img_arr[Vhalf:, :Split, :3]\n",
    "bot_img7 = img_arr[Vhalf:, Split:Split*2, :3]\n",
    "bot_img8 = img_arr[Vhalf:, Split*2:Split*3, :3]\n",
    "bot_img9 = img_arr[Vhalf:, Split*3:Split*4, :3]\n",
    "bot_img10 = img_arr[Vhalf:, Split*4:Split*5, :3]\n",
    "\n",
    "#displaying these 10 different parts of images in respective  subplot axis\n",
    "fig, axis = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axis[0,0].set_title('Image 1',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0,0].imshow(top_img1)\n",
    "\n",
    "axis[0,1].set_title('Image 2',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0,1].imshow(top_img2)\n",
    "\n",
    "axis[0,2].set_title('Image 3',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0,2].imshow(top_img3)\n",
    "\n",
    "axis[0,3].set_title('Image 4',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0,3].imshow(top_img4)\n",
    "\n",
    "axis[0,4].set_title('Image 5',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0,4].imshow(top_img5)\n",
    "\n",
    "axis[1,0].set_title('Image 6',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1,0].imshow(bot_img6)\n",
    "\n",
    "axis[1,1].set_title('Image 7',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1,1].imshow(bot_img7)\n",
    "\n",
    "axis[1,2].set_title('Image 8',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1,2].imshow(bot_img8)\n",
    "\n",
    "axis[1,3].set_title('Image 9',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1,3].imshow(bot_img9)\n",
    "\n",
    "axis[1,4].set_title('Image 10',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1,4].imshow(bot_img10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYxLSwSWrgey"
   },
   "source": [
    "## Q4 (2 Points)\n",
    "imgGray = ** 0.2989 * R + 0.5870 * G + 0.1140 * B **. \n",
    "\n",
    "this is the formula to convert any colored images to greyscale. \n",
    "\n",
    "\n",
    "Convert \"first\" into greyscale using above formula and store it in \"first_bw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:03:46.998677Z",
     "iopub.status.busy": "2022-05-24T12:03:46.998356Z",
     "iopub.status.idle": "2022-05-24T12:03:47.214159Z",
     "shell.execute_reply": "2022-05-24T12:03:47.213351Z",
     "shell.execute_reply.started": "2022-05-24T12:03:46.998637Z"
    },
    "id": "482aUQZeraMd"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "first = img_arr[:500, :290, :]\n",
    "\n",
    "\n",
    "## Write your code here\n",
    "#Storing the RGB values to the variables B, G, R\n",
    "B, G, R = first[:,:,0], first[:,:,1], first[:,:,2]\n",
    "#formula to convert RGBtoGray Image\n",
    "imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n",
    "#storing the Grayscale image array values in first_bw variable\n",
    "first_bw = imgGray\n",
    "#displaying the converted Grayscale Image\n",
    "plt.figure(figsize = (5,10))\n",
    "plt.imshow(first_bw, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiStl1oc6O5O"
   },
   "source": [
    "## Q5 (6 Points)\n",
    "Find out the effects of kernel1 and kernel2 on the \"first\" using cv2.filter2D function. \n",
    "\n",
    "Plot and State the difference between effect1 and effect2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:24:43.752923Z",
     "iopub.status.busy": "2022-05-24T12:24:43.752623Z",
     "iopub.status.idle": "2022-05-24T12:24:44.169591Z",
     "shell.execute_reply": "2022-05-24T12:24:44.168663Z",
     "shell.execute_reply.started": "2022-05-24T12:24:43.752891Z"
    },
    "id": "TbY77UkX67Xv"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "kernel1 = np.array([[-1,-1,-1],\n",
    "                    [0,0,0],\n",
    "                    [1,1,1]])\n",
    "\n",
    "kernel2 = kernel1.transpose()\n",
    "\n",
    "\n",
    "effect1 = cv2.filter2D(src = first, kernel = kernel1, ddepth = -1)\n",
    "effect2 = cv2.filter2D(src = first, kernel = kernel2, ddepth = -1)\n",
    "\n",
    "# plot effect1 and effect2 using plt.subplots\n",
    "fig, axis = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axis[0].set_title('Image Effect 1',fontweight = \"bold\",fontsize = 20)\n",
    "axis[0].imshow(effect1)\n",
    "\n",
    "axis[1].set_title('Image Effect 2',fontweight = \"bold\",fontsize = 20)\n",
    "axis[1].imshow(effect2)\n",
    "\n",
    "\n",
    "\n",
    "# state the difference\n",
    "\n",
    "difference = ''' \n",
    "The kernal is looks similar to Sobel filter \n",
    "\n",
    "The difference is the edge detection with respect to y-axis top to bottom portion(Vertical changes in pixel values) in Image effect 1\n",
    "and Horizontal Changes i.e with respect to x-axis Left to right portion in Image effect 2 after applying kernal1 with the src image.\n",
    "\n",
    "In Image Effect 1 ,the bottom portion edges are more precise than the top portion(Vertical changes) with respect to the kernel1 used\n",
    "whereas in the Image Effect 2, as the kernal2 is the transpose of the kernal1 that show the difference in such a way that \n",
    "the left portion edges are more precise than right portion edges(Horizontal Changes).\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "print(difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh-Tu3DrGf7Y"
   },
   "source": [
    "## Q6 (4 Points)\n",
    "\n",
    "State differences between -  \n",
    "\n",
    "> Feature extraction and finetuning in transfer learning.\n",
    "\n",
    "> Hidden layer in a neural network and Head layer in transfer learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:25:02.894786Z",
     "iopub.status.busy": "2022-05-24T12:25:02.894495Z",
     "iopub.status.idle": "2022-05-24T12:25:02.901704Z",
     "shell.execute_reply": "2022-05-24T12:25:02.900856Z",
     "shell.execute_reply.started": "2022-05-24T12:25:02.894754Z"
    },
    "id": "R1ja-IvR4d34"
   },
   "outputs": [],
   "source": [
    "# start your answer here\n",
    "\n",
    "answer1 = '''\n",
    "In finetuning, we start with a pretrained model and update all of the model's parameters for our new task,\n",
    "in essence retraining the whole model. In feature extraction, we start with a pretrained model and only\n",
    "update the final layer weights from which we derive predictions.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "answer2 = '''\n",
    "In neural networks, a hidden layer is located between the input and output of the algorithm, in which \n",
    "the function applies weights to the inputs and directs them through an activation function as the output.\n",
    "In short, the hidden layers perform nonlinear transformations of the inputs entered into the network.\n",
    "\n",
    "Transfer Learning is the approach of making use of an already trained deep neural network part (with Dense layers)\n",
    "is called the head layers\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "print(answer1)\n",
    "print()\n",
    "print(answer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7S2hsT-7eu-"
   },
   "source": [
    "## Q7 (1 point)\n",
    "\n",
    "Write the formula for IOU score using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T12:29:20.595478Z",
     "iopub.status.busy": "2022-05-24T12:29:20.595111Z",
     "iopub.status.idle": "2022-05-24T12:29:20.603948Z",
     "shell.execute_reply": "2022-05-24T12:29:20.602974Z",
     "shell.execute_reply.started": "2022-05-24T12:29:20.595432Z"
    }
   },
   "outputs": [],
   "source": [
    "#code Example to calculate IOU score using Numpy \n",
    "import numpy as np\n",
    "target = np.array([[0,1,1],[0,1,1],[0,1,1]], dtype=bool)\n",
    "prediction = np.array([[1,1,0],[1,1,0],[1,1,0]], dtype=bool)\n",
    "intersection = np.logical_and(target, prediction)\n",
    "union = np.logical_or(target, prediction)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "\n",
    "print(f'The IOU Score is : {iou_score}')\n",
    "print(\"Th IOU Score Formula: iou_score = np.sum(intersection) / np.sum(union) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UXaXUjhhCvz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mL9NMduQFg5"
   },
   "source": [
    "## Q8 (3 points)\n",
    "\n",
    "For below model, \n",
    "\n",
    "For a Convolutional layer in neural network, below values are given\n",
    "\n",
    "> Input shape = (64, 64, 3)\n",
    "\n",
    "> Convolutional filter size = (3,3)\n",
    "\n",
    "> Number of Conv Filters = 16\n",
    "\n",
    "> padding = \"same\"\n",
    "\n",
    "> stride = 1\n",
    "\n",
    "\n",
    "Find out the **output shape** after this layer, **number of total trainable paramaters** and **computational cost**.\n",
    "\n",
    "Hint: Total trainable parameters is the sum of total number of weights and biases in the layer.\n",
    "\n",
    "Hint: Computational cost is the total number of matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUVQCUDXP_Rm"
   },
   "outputs": [],
   "source": [
    "from keras.engine.sequential import Sequential\n",
    "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "                    Input(shape = (500, 290, 3)),\n",
    "                    Conv2D(16, (3,3), activation = 'relu'),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Conv2D(32, (3,3), activation = 'relu'),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Conv2D(64, (3,3), activation = 'relu'),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    Dense(256, activation = \"relu\"),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(5, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "# write your answer here\n",
    "\n",
    "out_shape = \n",
    "\n",
    "train_params = \n",
    "\n",
    "comp_cost = # write a function for this looping over layers, calculating for each layer\n",
    "\n",
    "print(\"Output shape:\", out_shape)\n",
    "print(\"Trainable parameters:\", train_params)\n",
    "print(\"Computational Cost:\", comp_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gL2_7F2jxec"
   },
   "source": [
    "## Q9 (5 points)\n",
    "\n",
    "1. Write down the name of the model that used dropout for preventing overfitting for the first time?\n",
    "\n",
    "2. How did GoogleNet manage to reduce its number of parameters despite having a deep heirarchy of conv layers?\n",
    "\n",
    "3. Which network made 3-by-3 filters state-of-the-art?\n",
    "\n",
    "4. How does VGGNet ensure growing depth despit shrinking spatial dimensions deep into the network hierarchy?\n",
    "\n",
    "5. What does Deconvnet do? Which network introduced it? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJMt9xKSTQsf"
   },
   "source": [
    "## Q10 (20 points)\n",
    "\n",
    "Use the Iris dataset to classify iris flowers into three classes of species.\n",
    "https://www.kaggle.com/datasets/uciml/iris \n",
    "\n",
    "Develop a transfer learning network by following these steps:\n",
    "1. Derive a base model using VGGNet - any configuration of your choice. \n",
    "\n",
    "2. Augment your data set by defining data augmentation layers - use at least 2 different augmentation techniques.\n",
    "\n",
    "3. Build your network with the augmentation layers and the base model from VGGNet.\n",
    "\n",
    "4. Add a pooling layer and a dense layer at the end to get a single prediction per image.\n",
    "\n",
    "5. Compile your model using categorical cross entropy loss, adam optimizer, and accuracy metric.\n",
    "\n",
    "6. Train your network twice - once with dropout and another time without dropout. \n",
    "\n",
    "7. Plot training and validation losses and accuracies for both cases of point #6. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIiqmkDHRLwC"
   },
   "source": [
    "## Thank you for completing all the questions!\n",
    "#### Download your notebook and submit it in the google form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whmuWZ5AlMzL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
