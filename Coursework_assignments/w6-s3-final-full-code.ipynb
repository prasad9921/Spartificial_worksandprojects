{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Building and Training a UNet Model (W6 S3)","metadata":{"id":"xksEYwr8QXgN"}},{"cell_type":"markdown","source":"### About the Dataset\n\nThis dataset contains 9766 realistic renders of lunar landscapes and their masks (segmented into three classes: sky, small rocks, bigger rocks). Additionally, a csv file of bounding boxes and cleaned masks of ground truths are provided.\n\nAn interesting feature of this dataset is that the images are synthetic; they were created using Planetside Software's Terragen. This isn't too obvious immediately as the renderings are highly realistic but it does make more sense after taking into account the scarcity of space imagery data.\n\nAcknowledgment: Romain Pessia and Genya Ishigami of the Space Robotics Group, Keio University, Japan. You can find the datasetÂ https://www.kaggle.com/romainpessia/artificial-lunar-rocky-landscape-dataset","metadata":{"id":"jcPmSyZzQXgP"}},{"cell_type":"markdown","source":"### Reminder to turn on your GPU accelerator, from right hand side of your kaggle notebook, under Settings.","metadata":{"id":"9XMsRC-aTdIi"}},{"cell_type":"markdown","source":"### Importing libraries\n\n","metadata":{"id":"vneA5pJxQXgR"}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras \n\nfrom tqdm import tqdm\nimport glob\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"bGIcq3_DQXgT","execution":{"iopub.status.busy":"2022-05-27T13:42:31.813301Z","iopub.execute_input":"2022-05-27T13:42:31.814176Z","iopub.status.idle":"2022-05-27T13:42:38.374843Z","shell.execute_reply.started":"2022-05-27T13:42:31.814019Z","shell.execute_reply":"2022-05-27T13:42:38.373201Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Installing segmentation_models module\n\nMake sure to turn your internet on from kaggle settings on the right hand side. If you don't see the Internet option in Settings, verify your kaggle profile by updating you phone number. \n\n### you'll know more about segmentation_models at the end of this lecture and in the next lecture. In this lecture we'll only use segmentation_models for iou_score. ","metadata":{"id":"1GJtQO3zTdIl"}},{"cell_type":"code","source":"\n!pip install segmentation_models\n\nimport segmentation_models as sm\n\nos.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\nsm.set_framework('tf.keras')\nkeras.backend.set_image_data_format('channels_last')","metadata":{"id":"0R7PLw5gTdIl","outputId":"a21c7f61-0386-418c-b2c2-97f4de399e1f","execution":{"iopub.status.busy":"2022-05-27T13:42:38.376582Z","iopub.execute_input":"2022-05-27T13:42:38.376915Z","iopub.status.idle":"2022-05-27T13:42:50.928328Z","shell.execute_reply.started":"2022-05-27T13:42:38.376878Z","shell.execute_reply":"2022-05-27T13:42:50.927468Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Pipeline","metadata":{"id":"OdS9NTtwQXgV"}},{"cell_type":"code","source":"'''\nHere load_data function is called. This will load the dataset paths and \nsplit it into X_train, X_test, y_train, y_test '''\n\nimg_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/render'\nmask_dir = '../input/artificial-lunar-rocky-landscape-dataset/images/clean'\n\n\n# let's get the list of image paths and mask paths in sorted order from the given directory respectively\nimages = [os.path.join(img_dir, x) for x in sorted(os.listdir(img_dir))]\nmasks = [os.path.join(mask_dir, x) for x in sorted(os.listdir(mask_dir))]","metadata":{"id":"SJ1vD73xTdIp","outputId":"7f9ef7f4-0362-474e-cb2b-c211e5b82eb6","execution":{"iopub.status.busy":"2022-05-27T13:42:50.930128Z","iopub.execute_input":"2022-05-27T13:42:50.930385Z","iopub.status.idle":"2022-05-27T13:42:51.359499Z","shell.execute_reply.started":"2022-05-27T13:42:50.930354Z","shell.execute_reply":"2022-05-27T13:42:51.358535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# let's get top 5 images and masks from the lists\nimages[:5], masks[:5]","metadata":{"id":"dj-4R4sbTdIq","execution":{"iopub.status.busy":"2022-05-27T13:42:51.361696Z","iopub.execute_input":"2022-05-27T13:42:51.361951Z","iopub.status.idle":"2022-05-27T13:42:51.369522Z","shell.execute_reply.started":"2022-05-27T13:42:51.361920Z","shell.execute_reply":"2022-05-27T13:42:51.368890Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## For this session, we will just use first 2000 images and masks as our dataset","metadata":{"id":"4PjqMlM-TdIr"}},{"cell_type":"code","source":"images = images[:2000]\nmasks = masks[:2000]","metadata":{"id":"oZrnkN5XTdIr","execution":{"iopub.status.busy":"2022-05-27T13:42:51.370636Z","iopub.execute_input":"2022-05-27T13:42:51.370860Z","iopub.status.idle":"2022-05-27T13:42:51.381862Z","shell.execute_reply.started":"2022-05-27T13:42:51.370832Z","shell.execute_reply":"2022-05-27T13:42:51.381210Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sample_img = Image.open(images[0])\nsample_img","metadata":{"id":"6EXl8jnlTdIs","execution":{"iopub.status.busy":"2022-05-27T13:42:51.383273Z","iopub.execute_input":"2022-05-27T13:42:51.383676Z","iopub.status.idle":"2022-05-27T13:42:51.585171Z","shell.execute_reply.started":"2022-05-27T13:42:51.383632Z","shell.execute_reply":"2022-05-27T13:42:51.584435Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sample_mask = Image.open(masks[0])\nsample_mask","metadata":{"id":"DaZRbOJFTdIs","execution":{"iopub.status.busy":"2022-05-27T13:42:51.586124Z","iopub.execute_input":"2022-05-27T13:42:51.586326Z","iopub.status.idle":"2022-05-27T13:42:51.612406Z","shell.execute_reply.started":"2022-05-27T13:42:51.586302Z","shell.execute_reply":"2022-05-27T13:42:51.611547Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sample_img.size","metadata":{"id":"6FyHq7sYTdIs","execution":{"iopub.status.busy":"2022-05-27T13:42:51.613613Z","iopub.execute_input":"2022-05-27T13:42:51.613851Z","iopub.status.idle":"2022-05-27T13:42:51.619533Z","shell.execute_reply.started":"2022-05-27T13:42:51.613823Z","shell.execute_reply":"2022-05-27T13:42:51.618765Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Originally, our images size is (720, 480) but we will reduce the size for better and faster processing. Since we are focusing on the clean masks so it will not effect much. \n\n### Ground masks are more detailed and have so much noise. We'll keep things easy for our lecture. However, feel free to use ground masks and play around to explore more. ","metadata":{"id":"akC9kBTmTdIt"}},{"cell_type":"code","source":"# we will use this shape of data for our model\nH = 256 # height of image\nW = 256 # width of image\n\n\n# for images and labels, to store our dataset\nX_img = []\ny_mask = []\n\n# here we have our loop to read, process and store our images X_img, and y_mask variables\nfor x, y in tqdm(zip(images, masks)):\n    # reading images\n    img = cv2.imread(x, cv2.IMREAD_COLOR)\n    img = cv2.resize(img, (W, H))\n    img = img / 255.0\n    img = img.astype(np.float32) # if pixel values are less than 1, then it is important that the values have float datatype\n    \n    # reading masks\n    mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n    mask = cv2.resize(mask, (W, H))\n    mask = mask.astype(np.int32)  # if pixel values are between 1 and 255, then it is important that the values have integer datatype\n    \n    # storing images and masks\n    X_img.append(img)\n    y_mask.append(mask)\n","metadata":{"id":"-r412gybTdIt","execution":{"iopub.status.busy":"2022-05-27T13:42:51.620950Z","iopub.execute_input":"2022-05-27T13:42:51.621541Z","iopub.status.idle":"2022-05-27T13:43:46.768913Z","shell.execute_reply.started":"2022-05-27T13:42:51.621505Z","shell.execute_reply":"2022-05-27T13:43:46.768182Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_img = np.array(X_img)\ny_mask = np.array(y_mask)\n\n# 1600 datapoints as training dataset and 400 for validation dataset \nX_train = X_img[:1600]\nX_valid = X_img[1600:]\n\ny_train = y_mask[:1600]\ny_valid = y_mask[1600:]\n","metadata":{"id":"7GFo6HUcTdIu","execution":{"iopub.status.busy":"2022-05-27T13:43:46.771044Z","iopub.execute_input":"2022-05-27T13:43:46.771749Z","iopub.status.idle":"2022-05-27T13:43:48.040620Z","shell.execute_reply.started":"2022-05-27T13:43:46.771713Z","shell.execute_reply":"2022-05-27T13:43:48.039687Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"X_train.shape\n# number of images, height, width, channels","metadata":{"id":"qD4Nzq55TdIu","execution":{"iopub.status.busy":"2022-05-27T13:43:48.042014Z","iopub.execute_input":"2022-05-27T13:43:48.042343Z","iopub.status.idle":"2022-05-27T13:43:48.049316Z","shell.execute_reply.started":"2022-05-27T13:43:48.042297Z","shell.execute_reply":"2022-05-27T13:43:48.048341Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 10))\n\nax1.set_title('Image')\nax2.set_title('Mask')\n\nax1.imshow(X_train[1])\nax2.imshow(y_train[1], cmap = 'gray')\n\nplt.show()","metadata":{"id":"8wxdt07iTdIv","execution":{"iopub.status.busy":"2022-05-27T13:43:48.050973Z","iopub.execute_input":"2022-05-27T13:43:48.051541Z","iopub.status.idle":"2022-05-27T13:43:48.394086Z","shell.execute_reply.started":"2022-05-27T13:43:48.051493Z","shell.execute_reply":"2022-05-27T13:43:48.392988Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Check this article to know more about how to build optimized data pipeline using tf\nhttps://www.tensorflow.org/guide/data_performance","metadata":{"id":"5WP9p5NATdIv"}},{"cell_type":"markdown","source":"# Data Pipeline\n\n### One hot encoding\n\n![](https://i.imgur.com/mtimFxh.png)\n\n#### Similarly, we'll one hot encode our labels to 4 different channels for four classes","metadata":{"id":"ANA5KGoRTdIx"}},{"cell_type":"code","source":"batch_size = 4\nnum_classes = 4 \n\n'''Here the from_tensor_slices function is called to make dataset objects of our training and validation sets'''\n# calling tf_dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, tf.one_hot(y_train, num_classes, dtype=tf.int32)))\n\nvalid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, tf.one_hot(y_valid, num_classes, dtype=tf.int32)))\n","metadata":{"id":"GEFDAxgITdIx","execution":{"iopub.status.busy":"2022-05-27T13:43:48.395478Z","iopub.execute_input":"2022-05-27T13:43:48.395785Z","iopub.status.idle":"2022-05-27T13:43:52.766650Z","shell.execute_reply.started":"2022-05-27T13:43:48.395747Z","shell.execute_reply":"2022-05-27T13:43:52.765710Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Read more about prefetching and AUTOTUNE here: https://www.tensorflow.org/guide/data_performance#optimize_performance\n\n## Naive Approach\n![](https://www.tensorflow.org/guide/images/data_performance/naive.svg)\n\n\n## After prefetching\n\n![](https://www.tensorflow.org/guide/images/data_performance/prefetched.svg)","metadata":{"id":"MA6xzMK3TdIx"}},{"cell_type":"code","source":"train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"id":"KiLkS-hZTdIx","execution":{"iopub.status.busy":"2022-05-27T13:43:52.768275Z","iopub.execute_input":"2022-05-27T13:43:52.768573Z","iopub.status.idle":"2022-05-27T13:43:52.779076Z","shell.execute_reply.started":"2022-05-27T13:43:52.768532Z","shell.execute_reply":"2022-05-27T13:43:52.777468Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sample = iter(valid_dataset)\ndata = next(sample)\ndata[0].shape\n# batch size, height, width, channels","metadata":{"id":"lWIoKssMTdIx","execution":{"iopub.status.busy":"2022-05-27T13:43:52.780237Z","iopub.execute_input":"2022-05-27T13:43:52.780468Z","iopub.status.idle":"2022-05-27T13:43:53.152356Z","shell.execute_reply.started":"2022-05-27T13:43:52.780443Z","shell.execute_reply":"2022-05-27T13:43:53.151486Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data[1].shape\n# batch size, height, width, channels/classes","metadata":{"id":"SNNLqwmITdIy","execution":{"iopub.status.busy":"2022-05-27T13:43:53.153725Z","iopub.execute_input":"2022-05-27T13:43:53.153933Z","iopub.status.idle":"2022-05-27T13:43:53.162908Z","shell.execute_reply.started":"2022-05-27T13:43:53.153907Z","shell.execute_reply":"2022-05-27T13:43:53.162331Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Creating U-net Architecture","metadata":{"id":"1MqxtDTmQXga"}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\nfrom tensorflow.keras.models import Model\n\n'''conv_block it is used to create one block with two convolution layer \nfollowed by BatchNormalization and activation function relu. \nIf the pooling is required then Maxpool2D is applied and return it else not.'''\n# function to create convolution block\ndef conv_block(inputs, filters, pool=True):\n    x = Conv2D(filters, 3, padding=\"same\")(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    if pool == True:\n        p = MaxPool2D((2, 2))(x)\n        return x, p\n    else:\n        return x\n\n'''build_unet it is used to create the U-net architecture.'''\n# function to build U-net\ndef build_unet(shape, num_classes):\n    inputs = Input(shape)\n\n    \"\"\" Encoder \"\"\"\n    x1, p1 = conv_block(inputs, 16, pool=True)\n    x2, p2 = conv_block(p1, 32, pool=True)\n    x3, p3 = conv_block(p2, 48, pool=True)\n    x4, p4 = conv_block(p3, 64, pool=True)\n\n    \"\"\" Bridge \"\"\"\n    b1 = conv_block(p4, 128, pool=False)\n\n    \"\"\" Decoder \"\"\"\n    # Reference for UpSampling2D: https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\n    # it simply repeats the rows and columns of the data by size[0] and size[1] respectively in nearest interpolation\n    # check below in the below cell, the difference between bilinear and nearest interpolation\n    u1 = UpSampling2D(size = (2, 2), interpolation=\"bilinear\")(b1)\n    c1 = Concatenate()([u1, x4])\n    x5 = conv_block(c1, 64, pool=False)\n\n    u2 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x5)\n    c2 = Concatenate()([u2, x3])\n    x6 = conv_block(c2, 48, pool=False)\n\n    u3 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x6)\n    c3 = Concatenate()([u3, x2])\n    x7 = conv_block(c3, 32, pool=False)\n\n    u4 = UpSampling2D((2, 2), interpolation=\"bilinear\")(x7)\n    c4 = Concatenate()([u4, x1])\n    x8 = conv_block(c4, 16, pool=False)\n\n    \"\"\" Output layer \"\"\"\n    output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x8)\n\n    return Model(inputs, output)","metadata":{"id":"tYCyf8smQXga","execution":{"iopub.status.busy":"2022-05-27T13:43:53.164493Z","iopub.execute_input":"2022-05-27T13:43:53.164882Z","iopub.status.idle":"2022-05-27T13:43:53.182628Z","shell.execute_reply.started":"2022-05-27T13:43:53.164834Z","shell.execute_reply":"2022-05-27T13:43:53.181574Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"input_shape = (1, 3, 2, 1)\nx = np.arange(np.prod(input_shape)).reshape(input_shape)\nprint(x)\n\nb = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation = \"bilinear\")(x)\nprint(b)\n\nn = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation = \"nearest\")(x)\nprint(n)\n","metadata":{"id":"4D-NgErpUl_A","execution":{"iopub.status.busy":"2022-05-27T13:43:53.183874Z","iopub.execute_input":"2022-05-27T13:43:53.184774Z","iopub.status.idle":"2022-05-27T13:43:53.249913Z","shell.execute_reply.started":"2022-05-27T13:43:53.184726Z","shell.execute_reply":"2022-05-27T13:43:53.248829Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**For Contracting Path:** the **conv_block** function is called four time which will create four block with pooling (pool = True). The process is repeated 3 more times.\n\n**For Bridge:** the **conv_block** function is called one time without pooling (pool=False).\n\n**For Expansive Path: UpSampling2D** is used to expands the size of images. This expanded  image is concatenated with the corresponding image from the contracting path, The reason here is to combine the information from the previous layers in order to get a more precise prediction. And now **conv_block** function is called without pooling (pool=False). The process is repeated 3 more times.\n\nThe last step is to reshape the image to satisfy our prediction requirements. The last layer is a convolution layer with 1 filter of size 1x1.","metadata":{"id":"8wNXCjt4QXgb"}},{"cell_type":"code","source":"# calling build_unet function\nmodel = build_unet((256, 256, 3), 4)\nmodel.summary()","metadata":{"id":"65hPnreJQXgb","execution":{"iopub.status.busy":"2022-05-27T13:43:53.252970Z","iopub.execute_input":"2022-05-27T13:43:53.253555Z","iopub.status.idle":"2022-05-27T13:43:53.718847Z","shell.execute_reply.started":"2022-05-27T13:43:53.253516Z","shell.execute_reply":"2022-05-27T13:43:53.716693Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Load model and compile","metadata":{"id":"bMgeqmX2QXgc"}},{"cell_type":"code","source":"# importing libraries\nfrom segmentation_models.metrics import iou_score\n\n\"\"\" Hyperparameters \"\"\"\nlr = 1e-4\nepochs = 5\n\n\"\"\"Model\"\"\"\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=tf.keras.optimizers.Adam(lr), \n              metrics=[iou_score])\n\n\ntrain_steps = len(X_train)//batch_size\nvalid_steps = len(X_valid)//batch_size","metadata":{"id":"z91qV2ZwQXgc","execution":{"iopub.status.busy":"2022-05-27T13:43:53.720421Z","iopub.execute_input":"2022-05-27T13:43:53.720665Z","iopub.status.idle":"2022-05-27T13:43:53.738900Z","shell.execute_reply.started":"2022-05-27T13:43:53.720637Z","shell.execute_reply":"2022-05-27T13:43:53.738184Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Train model","metadata":{"id":"SBhRPBKPQXgc"}},{"cell_type":"code","source":"'''model.fit is used to train the model'''\nmodel_history = model.fit(train_dataset,\n        steps_per_epoch=train_steps,\n        validation_data=valid_dataset,\n        validation_steps=valid_steps,\n        epochs=epochs\n    )","metadata":{"id":"4lJgBNVwQXgd","execution":{"iopub.status.busy":"2022-05-27T13:43:53.739804Z","iopub.execute_input":"2022-05-27T13:43:53.740044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict from model","metadata":{"id":"9d0U7XJZQXgd"}},{"cell_type":"code","source":"# function to predict result \ndef predict_image(img_path, mask_path, model):\n    H = 256\n    W = 256\n    num_classes = 4\n\n    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n    img = cv2.resize(img, (W, H))\n    img = img / 255.0\n    img = img.astype(np.float32)\n\n    ## Read mask\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    mask = cv2.resize(mask, (W, H))   ## (256, 256)\n    mask = np.expand_dims(mask, axis=-1) ## (256, 256, 1)\n    mask = mask * (255/num_classes)\n    mask = mask.astype(np.int32)\n    mask = np.concatenate([mask, mask, mask], axis=2)\n\n    ## Prediction\n    pred_mask = model.predict(np.expand_dims(img, axis=0))[0]\n    pred_mask = np.argmax(pred_mask, axis=-1)\n    pred_mask = np.expand_dims(pred_mask, axis=-1)\n    pred_mask = pred_mask * (255/num_classes)\n    pred_mask = pred_mask.astype(np.int32)\n    pred_mask = np.concatenate([pred_mask, pred_mask, pred_mask], axis=2)\n\n    return img, mask, pred_mask","metadata":{"id":"ey2fgIyiQXge","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to display result\ndef display(display_list):\n    plt.figure(figsize=(12, 10))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask', 'Mask On Image']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n        plt.show()","metadata":{"id":"y_f35xF5QXge","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '../input/artificial-lunar-rocky-landscape-dataset/images/render/render0041.png'\nmask_path = '../input/artificial-lunar-rocky-landscape-dataset/images/clean/clean0041.png'\n\nimg, mask, pred_mask = predict_image(img_path, mask_path, model)\n\ndisplay([img, mask, pred_mask])","metadata":{"id":"rhP2yyG-QXge","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## segmentation_model\n\nsegmentation_models is a python library with Neural Networks for Image Segmentation based on Keras and TensorFlow.\n\nThe main features of this library are:\n\n* High level API (just two lines of code to create model for segmentation)\n* 4 models architectures for binary and multi-class image segmentation (including legendary Unet)\n* 25 available backbones for each architecture\n* All backbones have pre-trained weights for faster and better convergence\n* Helpful segmentation losses (Jaccard, Dice, Focal) and metrics (IoU, F-score)\n\nLearn more: https://segmentation-models.readthedocs.io/en/latest/tutorial.html","metadata":{"id":"I6qn3N2xTdI2"}},{"cell_type":"markdown","source":"## A practical note: different backbones in modern U-Nets\n\nSo far, you have looked at how the U-Net architecture was implemented in the original work by Ronneberger et al. Over the years, many people have experienced with different setups for U-Nets, including pretraining on e.g. ImageNet and then finetuning to their specific image segmentation tasks.\n\nThis means that today, you will likely use a U-Net that no longer utilizes the original architecture as proposed above - but it's still a good starting point, because the contractive path, expansive path and the skip connections remain the same.\n\n**Common backbones for U-Net architectures these days are ResNet, ResNeXt, EfficientNet and DenseNet architectures. Often, these have been pretrained on the ImageNet dataset, so that many common features have already been learned. By using these backbone U-Nets, initialized with pretrained weights, it's likely that you can reach convergence on your segmentation problem much faster.**\n\nThat's it! You have now a high-level understanding of U-Net and its components sunglasses.\n\n## In the next session, we will learn how you can use segmentation_models using Transfer learning to use UNet architecture with different pretrained models as backbone.\n\n## In next week, we will learn about different techniques to improve the accuracy of our model. ","metadata":{"id":"9On3t8XsTdI2"}},{"cell_type":"code","source":"","metadata":{"id":"4TuEQnroQXgf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mBJIdnjXQXgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"rjQZ_xcAQXgf"},"execution_count":null,"outputs":[]}]}