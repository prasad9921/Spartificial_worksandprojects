{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Assignment 5 (AI4LR)","metadata":{"id":"elcpXOswLRgc"}},{"cell_type":"code","source":"Your_Name = \"V N D S R Prasad Jettiboina\"\nYour_Email_id = \"prasadjv99@gmail.com\"","metadata":{"id":"cfmCfF7eiTpk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Open this notebook in kaggle for easy access to the dataset and related libraries. ","metadata":{"id":"juQiIIMl4qVz"}},{"cell_type":"code","source":"# importing libraries\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image","metadata":{"id":"f5ayJkkRwkjb","execution":{"iopub.status.busy":"2022-06-12T08:33:31.133262Z","iopub.execute_input":"2022-06-12T08:33:31.134138Z","iopub.status.idle":"2022-06-12T08:33:31.138995Z","shell.execute_reply.started":"2022-06-12T08:33:31.134096Z","shell.execute_reply":"2022-06-12T08:33:31.138106Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Data \n\nTo get the dataset for this assignment, got to Add data option and search for https://www.kaggle.com/datasets/datatangai/people-with-occlusion-and-multipose-face-data in Search URL field.\n\nImport this dataset and there is only one image in this dataset. We are going to use this image only. ","metadata":{"id":"d72b-W8fFaKI"}},{"cell_type":"code","source":"img_path = \"../input/people-with-occlusion-and-multipose-face-data/3.png\"\n\nimg = Image.open(img_path)\n\nimg","metadata":{"id":"T808--eV4qV0","execution":{"iopub.status.busy":"2022-06-12T08:33:34.357091Z","iopub.execute_input":"2022-06-12T08:33:34.357915Z","iopub.status.idle":"2022-06-12T08:33:35.550845Z","shell.execute_reply.started":"2022-06-12T08:33:34.357876Z","shell.execute_reply":"2022-06-12T08:33:35.549955Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Q1 (1 Point) \n\nConvert the images \"img\" into an array and store that array into 'img_arr' variable.\n\nPlot the img_arr using plt\n","metadata":{"id":"QAhrBKqmjuHB"}},{"cell_type":"code","source":"# Complete the following code\n\n\nimg_arr = np.asarray(img)\nprint(type(img_arr),img_arr)\n\n# plot img_arr\n\nplt.imshow(img_arr)","metadata":{"id":"ShSSOgg350kW","execution":{"iopub.status.busy":"2022-06-12T08:33:41.565269Z","iopub.execute_input":"2022-06-12T08:33:41.566013Z","iopub.status.idle":"2022-06-12T08:33:41.906593Z","shell.execute_reply.started":"2022-06-12T08:33:41.565978Z","shell.execute_reply":"2022-06-12T08:33:41.905873Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Q2 (2 Points)\nRun the below code.\n\nDo you see anything abnormal with the shape of the above image? If yes, state the reason. ","metadata":{"id":"KqNymgaxpk1i"}},{"cell_type":"code","source":"print(\"Image shape:\", img_arr.shape)\n\n## print your reason here\n\nreason = \"The number of channels is 4\\nMeans the image format is of CMYK,RGBA etc (extra 4th channel)\"\n\nprint(reason)","metadata":{"id":"23zzRR39pj-X","execution":{"iopub.status.busy":"2022-06-12T08:33:46.209217Z","iopub.execute_input":"2022-06-12T08:33:46.209952Z","iopub.status.idle":"2022-06-12T08:33:46.215445Z","shell.execute_reply.started":"2022-06-12T08:33:46.209913Z","shell.execute_reply":"2022-06-12T08:33:46.214298Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Q3 (6 Points)\nThe above image \"img_arr\" is a combination of ten different images, separate all ten images using numpy and show all of them using plt.subplot.\n\n(the size and shape of individual images should remain the same)","metadata":{"id":"0wj839JEq2vN"}},{"cell_type":"code","source":"# first we are converting our four channels image into three channels image (easy to use and no extra complexities)\n'''Do not make any change in this below line of code.'''\nimg_arr = img_arr[:, :, :3]\n\n\n# Write your code here--------------------------->\n\n#Getting No of row and column values\nrow,col = img_arr.shape[0],img_arr.shape[1]\n#Vertically dividing the image into parts  \nr = row//2\n#Splittingting into 5 parts of image\nc = col//5\n\n#Using Numpy array value cting the above image in 10 different parts\nimg1 = img_arr[:r, :c, :3]\nimg2 = img_arr[:r, c:c*2, :3]\nimg3 = img_arr[:r, c*2:c*3, :3]\nimg4 = img_arr[:r, c*3:c*4, :3]\nimg5 = img_arr[:r, c*4:c*5, :3]\nimg6 = img_arr[r:, :c, :3]\nimg7 = img_arr[r:, c:c*2, :3]\nimg8 = img_arr[r:, c*2:c*3, :3]\nimg9 = img_arr[r:, c*3:c*4, :3]\nimg10 = img_arr[r:, c*4:c*5, :3]\n\n#displaying these 10 different parts of images in respective  subplot axis\nfig, axis = plt.subplots(2, 5, figsize=(15, 8))\naxis[0,0].set_title('Image 1',fontweight = \"bold\",fontsize = 20)\naxis[0,0].imshow(img1)\n\naxis[0,1].set_title('Image 2',fontweight = \"bold\",fontsize = 20)\naxis[0,1].imshow(img2)\n\naxis[0,2].set_title('Image 3',fontweight = \"bold\",fontsize = 20)\naxis[0,2].imshow(img3)\n\naxis[0,3].set_title('Image 4',fontweight = \"bold\",fontsize = 20)\naxis[0,3].imshow(img4)\n\naxis[0,4].set_title('Image 5',fontweight = \"bold\",fontsize = 20)\naxis[0,4].imshow(img5)\n\naxis[1,0].set_title('Image 6',fontweight = \"bold\",fontsize = 20)\naxis[1,0].imshow(img6)\n\naxis[1,1].set_title('Image 7',fontweight = \"bold\",fontsize = 20)\naxis[1,1].imshow(img7)\n\naxis[1,2].set_title('Image 8',fontweight = \"bold\",fontsize = 20)\naxis[1,2].imshow(img8)\n\naxis[1,3].set_title('Image 9',fontweight = \"bold\",fontsize = 20)\naxis[1,3].imshow(img9)\n\naxis[1,4].set_title('Image 10',fontweight = \"bold\",fontsize = 20)\naxis[1,4].imshow(img10)","metadata":{"id":"G979ANNupyoY","execution":{"iopub.status.busy":"2022-06-12T08:33:57.366405Z","iopub.execute_input":"2022-06-12T08:33:57.366749Z","iopub.status.idle":"2022-06-12T08:33:58.501151Z","shell.execute_reply.started":"2022-06-12T08:33:57.366721Z","shell.execute_reply":"2022-06-12T08:33:58.499123Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Q4 (2 Points)\nimgGray = ** 0.2989 * R + 0.5870 * G + 0.1140 * B **. \n\nthis is the formula to convert any colored images to greyscale. \n\n\nConvert \"first\" into greyscale using above formula and store it in \"first_bw\"","metadata":{"id":"wYxLSwSWrgey"}},{"cell_type":"code","source":"first = img_arr[:500, :290, :]\n\nplt.figure(figsize = (5,10))\nplt.imshow(first)\nplt.show()\n\n\n\n## Write your code here\nB, G, R = first[:,:,0], first[:,:,1], first[:,:,2]\n#formula to convert RGBtoGray Image\nimgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n#storing the Grayscale image array values in first_bw variable\nfirst_bw = imgGray\n#displaying the converted Grayscale Image\nplt.figure(figsize = (5,10))\nplt.imshow(first_bw, cmap='gray')","metadata":{"id":"482aUQZeraMd","execution":{"iopub.status.busy":"2022-06-12T08:34:03.753417Z","iopub.execute_input":"2022-06-12T08:34:03.753756Z","iopub.status.idle":"2022-06-12T08:34:04.242681Z","shell.execute_reply.started":"2022-06-12T08:34:03.753726Z","shell.execute_reply":"2022-06-12T08:34:04.241930Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Q5 (6 Points)\nFind out the effects of kernel1 and kernel2 on the \"first\" using cv2.filter2D function. \n\nPlot and State the difference between effect1 and effect2.\n","metadata":{"id":"kiStl1oc6O5O"}},{"cell_type":"code","source":"kernel1 = np.array([[-1,-1,-1],\n                    [0,0,0],\n                    [1,1,1]])\n\nkernel2 = kernel1.transpose()\n\n\neffect1 = cv2.filter2D(src = first, kernel = kernel1, ddepth = -1)\neffect2 = cv2.filter2D(src = first, kernel = kernel2, ddepth = -1)\n\n# plot effect1 and effect2 using plt.subplots\nfig, axis = plt.subplots(1, 2, figsize=(15, 8))\naxis[0].set_title('Image Effect 1',fontweight = \"bold\",fontsize = 20)\naxis[0].imshow(effect1)\n\naxis[1].set_title('Image Effect 2',fontweight = \"bold\",fontsize = 20)\naxis[1].imshow(effect2)\n\n# state the difference\n\ndifference = \"The Mask is used for edge detection similar to Sobel Filter.\\nKernel 1 will find horizontal edges because the zeros column is in the horizontal direction.\\nKernel 2 will find vertical edges because the zeros column is in the vertical direction.\" \nprint(difference)\n","metadata":{"id":"TbY77UkX67Xv","execution":{"iopub.status.busy":"2022-06-12T08:34:09.624001Z","iopub.execute_input":"2022-06-12T08:34:09.624433Z","iopub.status.idle":"2022-06-12T08:34:10.095442Z","shell.execute_reply.started":"2022-06-12T08:34:09.624396Z","shell.execute_reply":"2022-06-12T08:34:10.091257Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Q6 (4 Points)\n\nState differences between -  \n\n> Feature extraction and finetuning in transfer learning.\n\n> Hidden layer in a neural network and Head layer in transfer learning ","metadata":{"id":"Qh-Tu3DrGf7Y"}},{"cell_type":"code","source":"# start your answer here\n\nanswer1 = '''\nFeature extraction is used to extract features,CNNs isolate features and with these features model will decide (prediction).\nFinetuning is used to tune parameters to enhance model's performance.\nIn finetuning, we start with a pretrained model and update all of the model's parameters for our new task,\nin essence retraining the whole model. In feature extraction, we start with a pretrained model and only\nupdate the final layer weights from which we derive predictions.\n\n'''\n\n\nanswer2 = '''\nIn neural networks, a hidden layer is located between the input and output of the algorithm, in which \nthe function applies weights to the inputs and directs them through an activation function as the output.\nIn short, the hidden layers perform nonlinear transformations of the inputs entered into the network.\n\nTransfer Learning is the approach of making use of an already trained deep neural network part (with Dense layers)\nis called the head layers\n'''\n\nprint(answer1)\nprint()\nprint(answer2)","metadata":{"id":"R1ja-IvR4d34","execution":{"iopub.status.busy":"2022-06-12T08:34:13.843534Z","iopub.execute_input":"2022-06-12T08:34:13.844222Z","iopub.status.idle":"2022-06-12T08:34:13.850490Z","shell.execute_reply.started":"2022-06-12T08:34:13.844188Z","shell.execute_reply":"2022-06-12T08:34:13.849472Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Q7 (1 point)\n\nWrite the formula for IOU score using numpy.","metadata":{"id":"s7S2hsT-7eu-"}},{"cell_type":"code","source":"# Print your answer \nformula = \"np.sum(intersection) / np.sum(union)\"\n\n#code Example to calculate IOU score using Numpy \nimport numpy as np\ntarget = np.array([[0,1,1],[0,1,1],[0,1,1]], dtype=bool)\nprediction = np.array([[1,1,0],[1,1,0],[1,1,0]], dtype=bool)\nintersection = np.logical_and(target, prediction)\nunion = np.logical_or(target, prediction)\niou_score = np.sum(intersection) / np.sum(union)\n\nprint(f'The IOU Score is : {iou_score}')\nprint(\"Th IOU Score Formula: iou_score = np.sum(intersection) / np.sum(union) \")","metadata":{"id":"_qcETSSR8866","execution":{"iopub.status.busy":"2022-06-12T08:34:16.446158Z","iopub.execute_input":"2022-06-12T08:34:16.446878Z","iopub.status.idle":"2022-06-12T08:34:16.459830Z","shell.execute_reply.started":"2022-06-12T08:34:16.446834Z","shell.execute_reply":"2022-06-12T08:34:16.459063Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"7UXaXUjhhCvz"}},{"cell_type":"markdown","source":"## Q8 (3 points)\n\nFor below model, \n\nFor a Convolutional layer in neural network, below values are given\n\n> Input shape = (64, 64, 3)\n\n> Convolutional filter size = (3,3)\n\n> Number of Conv Filters = 16\n\n> padding = \"same\"\n\n> stride = 1\n\n\nFind out the **output shape** after this layer, **number of total trainable paramaters** and **computational cost**.\n\nHint: Total trainable parameters is the sum of total number of weights and biases in the layer.\n\nHint: Computational cost is the total number of matrix multiplications","metadata":{"id":"0mL9NMduQFg5"}},{"cell_type":"code","source":"from keras.engine.sequential import Sequential\nfrom keras.layers import Input, Conv2D, Dense, MaxPooling2D, Dropout\n\nmodel = Sequential([\n                    Input(shape = (500, 290, 3)),\n                    Conv2D(16, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Conv2D(32, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Conv2D(64, (3,3), activation = 'relu'),\n                    MaxPooling2D(2,2),\n                    Dense(256, activation = \"relu\"),\n                    Dropout(0.5),\n                    Dense(5, activation = \"sigmoid\")\n])\n\n# write your answer here\n\nout_shape = [None, 60, 34, 5]\n\ntrain_params = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n\n# write a function for this looping over layers, calculating for each layer\nip = [3, 16, 32, 64, 256] #Input channels\nop = [16, 32, 64, 256, 5] #Output channels\nf = 3 #filter size\ncomp_cost = 0\nfor i in range(5): #Output shape .,.,.,5\n    if type(model.layers[i]) == keras.layers.Conv2D:\n        n = ip[i] * f * f * op[i]        \n    else:\n        n = ip[i] * op[i]\n    b = op[i]\n    l = n + b\n    comp_cost += l\n\n\n\nprint(\"Output shape:\", out_shape)\nprint(\"Trainable parameters:\", train_params)\nprint(\"Computational Cost:\", comp_cost)","metadata":{"id":"HUVQCUDXP_Rm","execution":{"iopub.status.busy":"2022-06-12T08:35:19.133321Z","iopub.execute_input":"2022-06-12T08:35:19.134038Z","iopub.status.idle":"2022-06-12T08:35:19.210854Z","shell.execute_reply.started":"2022-06-12T08:35:19.134002Z","shell.execute_reply":"2022-06-12T08:35:19.210032Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Q9 (5 points)\n\n1. Write down the name of the model that used dropout for preventing overfitting for the first time?\n\n2. How did GoogleNet manage to reduce its number of parameters despite having a deep heirarchy of conv layers?\n\n3. Which network made 3-by-3 filters state-of-the-art?\n\n4. How does VGGNet ensure growing depth despit shrinking spatial dimensions deep into the network hierarchy?\n\n5. What does Deconvnet do? Which network introduced it? ","metadata":{"id":"1gL2_7F2jxec"}},{"cell_type":"code","source":"a='''\n1: AlexNet\n\n2: Using Inception layers and uses a filter(patch) size of 7x7, which is relatively large compared to other patch sizes within the network. This layer's primary purpose is to immediately reduce the input image, but not lose spatial information by utilising large filter sizes.\naverage pool converts 7x7x1024 featuremap to a 1x1x1024 featuremap.\n\n3: VGGNet\n\n4: The number of filters that we can use doubles on every step or through every stack of the convolution layer. This is a major principle used to design the architecture of the VGG16 network. One of the crucial downsides of the VGG16 network is that it is a huge network, which means that it takes more time to train its parameters.\n\nBecause of its depth and number of fully connected layers, the VGG16 model is more than 533MB. This makes implementing a VGG network a time-consuming task.\n\n5: Deconvet matches features to pixels that is exactly opposite of what a convolution does.\nIt was introduced by ZFNet\n'''\nprint(a)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:35:26.453023Z","iopub.execute_input":"2022-06-12T08:35:26.453518Z","iopub.status.idle":"2022-06-12T08:35:26.459973Z","shell.execute_reply.started":"2022-06-12T08:35:26.453469Z","shell.execute_reply":"2022-06-12T08:35:26.458547Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"## Q10 (20 points)\n\nUse the Iris dataset to classify iris flowers into three classes of species.\nhttps://www.kaggle.com/datasets/uciml/iris \n\nDevelop a transfer learning network by following these steps:\n1. Derive a base model using VGGNet - any configuration of your choice. \n\n2. Augment your data set by defining data augmentation layers - use at least 2 different augmentation techniques.\n\n3. Build your network with the augmentation layers and the base model from VGGNet.\n\n4. Add a pooling layer and a dense layer at the end to get a single prediction per image.\n\n5. Compile your model using categorical cross entropy loss, adam optimizer, and accuracy metric.\n\n6. Train your network twice - once with dropout and another time without dropout. \n\n7. Plot training and validation losses and accuracies for both cases of point #6. \n","metadata":{"id":"wJMt9xKSTQsf"}},{"cell_type":"code","source":"# getting path to our root directory\ndata_dir = '../input/iris-computer-vision'\n\nbatch_size = 32\nIMG_SIZE = (160, 160)\n\ntrain = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  shuffle=True,\n  seed=99,\n  validation_split=0.25,\n  subset=\"training\",\n  image_size=IMG_SIZE,\n  batch_size=batch_size)\n\nval = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  shuffle=True,\n  seed=99,\n  validation_split=0.25,\n  subset=\"validation\",\n  image_size=IMG_SIZE,\n  batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:06.440550Z","iopub.execute_input":"2022-06-12T08:26:06.441132Z","iopub.status.idle":"2022-06-12T08:26:09.744517Z","shell.execute_reply.started":"2022-06-12T08:26:06.441097Z","shell.execute_reply":"2022-06-12T08:26:09.743758Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.RandomFlip('horizontal'),\n  tf.keras.layers.RandomRotation(0.2),\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:16.144670Z","iopub.execute_input":"2022-06-12T08:26:16.145135Z","iopub.status.idle":"2022-06-12T08:26:16.190512Z","shell.execute_reply.started":"2022-06-12T08:26:16.145094Z","shell.execute_reply":"2022-06-12T08:26:16.189798Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"preprocess_input = tf.keras.applications.vgg19.preprocess_input","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:19.867404Z","iopub.execute_input":"2022-06-12T08:26:19.867763Z","iopub.status.idle":"2022-06-12T08:26:19.872820Z","shell.execute_reply.started":"2022-06-12T08:26:19.867734Z","shell.execute_reply":"2022-06-12T08:26:19.871331Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"rescale = tf.keras.layers.Rescaling(1./127.5, offset=-1)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:25.377353Z","iopub.execute_input":"2022-06-12T08:26:25.377690Z","iopub.status.idle":"2022-06-12T08:26:25.383114Z","shell.execute_reply.started":"2022-06-12T08:26:25.377663Z","shell.execute_reply":"2022-06-12T08:26:25.382373Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"IMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:26.984092Z","iopub.execute_input":"2022-06-12T08:26:26.984726Z","iopub.status.idle":"2022-06-12T08:26:27.960092Z","shell.execute_reply.started":"2022-06-12T08:26:26.984689Z","shell.execute_reply":"2022-06-12T08:26:27.959203Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"image_batch, label_batch = next(iter(train))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:30.440257Z","iopub.execute_input":"2022-06-12T08:26:30.440709Z","iopub.status.idle":"2022-06-12T08:26:38.162285Z","shell.execute_reply.started":"2022-06-12T08:26:30.440672Z","shell.execute_reply":"2022-06-12T08:26:38.161409Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"base_model.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:39.385564Z","iopub.execute_input":"2022-06-12T08:26:39.385914Z","iopub.status.idle":"2022-06-12T08:26:39.393281Z","shell.execute_reply.started":"2022-06-12T08:26:39.385883Z","shell.execute_reply":"2022-06-12T08:26:39.392450Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:41.842856Z","iopub.execute_input":"2022-06-12T08:26:41.843494Z","iopub.status.idle":"2022-06-12T08:26:41.857854Z","shell.execute_reply.started":"2022-06-12T08:26:41.843449Z","shell.execute_reply":"2022-06-12T08:26:41.856847Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:42.605519Z","iopub.execute_input":"2022-06-12T08:26:42.605857Z","iopub.status.idle":"2022-06-12T08:26:42.626836Z","shell.execute_reply.started":"2022-06-12T08:26:42.605829Z","shell.execute_reply":"2022-06-12T08:26:42.625950Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:45.465465Z","iopub.execute_input":"2022-06-12T08:26:45.466296Z","iopub.status.idle":"2022-06-12T08:26:45.664818Z","shell.execute_reply.started":"2022-06-12T08:26:45.466257Z","shell.execute_reply":"2022-06-12T08:26:45.663946Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:46.747795Z","iopub.execute_input":"2022-06-12T08:26:46.748169Z","iopub.status.idle":"2022-06-12T08:26:46.767939Z","shell.execute_reply.started":"2022-06-12T08:26:46.748138Z","shell.execute_reply":"2022-06-12T08:26:46.767211Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:26:48.508279Z","iopub.execute_input":"2022-06-12T08:26:48.508983Z","iopub.status.idle":"2022-06-12T08:26:48.515720Z","shell.execute_reply.started":"2022-06-12T08:26:48.508944Z","shell.execute_reply":"2022-06-12T08:26:48.514311Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"initial_epochs = 10\nhistory = model.fit(train,\n                    epochs=initial_epochs,\n                    validation_data=val)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:29:26.699380Z","iopub.execute_input":"2022-06-12T08:29:26.699912Z","iopub.status.idle":"2022-06-12T08:29:37.805616Z","shell.execute_reply.started":"2022-06-12T08:29:26.699865Z","shell.execute_reply":"2022-06-12T08:29:37.804843Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:29:40.372411Z","iopub.execute_input":"2022-06-12T08:29:40.372763Z","iopub.status.idle":"2022-06-12T08:29:40.655659Z","shell.execute_reply.started":"2022-06-12T08:29:40.372733Z","shell.execute_reply":"2022-06-12T08:29:40.654899Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(160, 160, 3))\nz = data_augmentation(inputs)\nz = preprocess_input(z)\nz = base_model(z, training=False)\nz = global_average_layer(z)\nz = tf.keras.layers.Dropout(0.2)(z)\noutputs = prediction_layer(z)\nmodel_dropout = tf.keras.Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:29:44.242398Z","iopub.execute_input":"2022-06-12T08:29:44.242757Z","iopub.status.idle":"2022-06-12T08:29:44.378289Z","shell.execute_reply.started":"2022-06-12T08:29:44.242727Z","shell.execute_reply":"2022-06-12T08:29:44.377513Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"base_learning_rate_dropout = 0.0001\nmodel_dropout.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate_dropout),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:29:47.900839Z","iopub.execute_input":"2022-06-12T08:29:47.901209Z","iopub.status.idle":"2022-06-12T08:29:47.912799Z","shell.execute_reply.started":"2022-06-12T08:29:47.901178Z","shell.execute_reply":"2022-06-12T08:29:47.912013Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"initial_epochs = 10\nhistory_dropout = model_dropout.fit(train,\n                    epochs=initial_epochs,\n                    validation_data=val)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:29:52.081189Z","iopub.execute_input":"2022-06-12T08:29:52.082010Z","iopub.status.idle":"2022-06-12T08:30:04.587347Z","shell.execute_reply.started":"2022-06-12T08:29:52.081970Z","shell.execute_reply":"2022-06-12T08:30:04.586566Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"acc = history_dropout.history['accuracy']\nval_acc = history_dropout.history['val_accuracy']\n\nloss = history_dropout.history['loss']\nval_loss = history_dropout.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T08:30:09.474455Z","iopub.execute_input":"2022-06-12T08:30:09.474804Z","iopub.status.idle":"2022-06-12T08:30:09.959492Z","shell.execute_reply.started":"2022-06-12T08:30:09.474773Z","shell.execute_reply":"2022-06-12T08:30:09.958716Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for completing all the questions!\n#### Download your notebook and submit it in the google form","metadata":{"id":"wIiqmkDHRLwC"}},{"cell_type":"code","source":"","metadata":{"id":"whmuWZ5AlMzL"},"execution_count":null,"outputs":[]}]}